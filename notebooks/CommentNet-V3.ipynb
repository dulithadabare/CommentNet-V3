{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CommentNet-V3 \n",
    "\n",
    "Welcome to the version 3 of CommentNet!\n",
    "\n",
    "First let's import the relevant packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/dulithadabare/anaconda3/envs/comment-net-nomkl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/dulithadabare/anaconda3/envs/comment-net-nomkl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/dulithadabare/anaconda3/envs/comment-net-nomkl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/dulithadabare/anaconda3/envs/comment-net-nomkl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/dulithadabare/anaconda3/envs/comment-net-nomkl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/dulithadabare/anaconda3/envs/comment-net-nomkl/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/dulithadabare/anaconda3/envs/comment-net-nomkl/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/dulithadabare/anaconda3/envs/comment-net-nomkl/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/dulithadabare/anaconda3/envs/comment-net-nomkl/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/dulithadabare/anaconda3/envs/comment-net-nomkl/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/dulithadabare/anaconda3/envs/comment-net-nomkl/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/dulithadabare/anaconda3/envs/comment-net-nomkl/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0818 14:47:55.275770 4613428672 __init__.py:71] TensorFlow version 1.14.0 detected. Last version known to be fully compatible is 1.13.1 .\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import string\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import f1_score\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation, Bidirectional, Concatenate, Dot, RepeatVector\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "import keras.backend as K\n",
    "import coremltools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's process the data set.\n",
    "\n",
    "We will process the data set and create a new data set by removing punctuation and trimming the lines in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(file_list_with_labels, output='../data/data_set.txt', output_label = '../data/labels.txt'):\n",
    "    phrase = []\n",
    "    emoji = []\n",
    "\n",
    "    with open (output, 'w') as output_file, open(output_label, 'w') as label_file:\n",
    "        \n",
    "        for filename, label in file_list_with_labels:\n",
    "            print('Processing file: ', filename)\n",
    "            with open (filename) as f:\n",
    "                  for line in f:\n",
    "                    line = line.strip()\n",
    "                    if len(line)>0:\n",
    "                        table = str.maketrans({key: None for key in string.punctuation})\n",
    "                        line = line.translate(table)\n",
    "                        output_file.write(line + '\\r\\n')\n",
    "                        label_file.write(label + '\\r\\n')\n",
    "                        phrase.append(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data set files are in 3 separate sets of files.\n",
    "\n",
    "- troll data set and it's labels\n",
    "- constructive data set and it's labels\n",
    "- positive data set and it's labels\n",
    "\n",
    "We will process these 3 separate data sets and create one combined dataset and label set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file:  ../data/troll.txt\n",
      "Processing file:  ../data/constructive.txt\n",
      "Processing file:  ../data/positive.txt\n"
     ]
    }
   ],
   "source": [
    "file_list_with_labels = []\n",
    "file_list_with_labels.append(('../data/troll.txt', '1'))\n",
    "file_list_with_labels.append(('../data/constructive.txt', '0'))\n",
    "file_list_with_labels.append(('../data/positive.txt', '0'))\n",
    "process_data(file_list_with_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, it's time to load the GloVe word embedding files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r') as f, open('../data/glove_word_index.txt', 'w') as word_index_file:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        word_index = []\n",
    "        for w in sorted(words):\n",
    "            word_index.append(w + ' ' + str(i))\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "        \n",
    "        for line in word_index:\n",
    "            word_index_file.write(line + '\\n')\n",
    "    return words_to_index, index_to_words, word_to_vec_map, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map, unknown_word_index = read_glove_vecs('../../../CommentNetData/glove.6B/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will also read in a custom word embedding to be used for unkown word tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_unknown_vecs(unknown_file):\n",
    "    \n",
    "    unknown_vector = None\n",
    "    \n",
    "    with open(unknown_file, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            unknown_vector = np.array(line[0:], dtype=np.float64)\n",
    "    \n",
    "    print('Unknown word vector is', unknown_vector.shape)\n",
    "\n",
    "    return unknown_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown word vector is (50,)\n"
     ]
    }
   ],
   "source": [
    "unkown_word_vector = read_unknown_vecs('../data/unknown_word_vector.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the index of ea in the vocabulary is 132265\n",
      "the 18th word in the vocabulary is #cccccc\n",
      "400001\n",
      "(1, 50)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.21020451]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"ea\"\n",
    "index = 18\n",
    "print(\"the index of\", word, \"in the vocabulary is\", word_to_index[word])\n",
    "print(\"the\", str(index) + \"th word in the vocabulary is\", index_to_word[index])\n",
    "print(unknown_word_index)\n",
    "print(word_to_vec_map[word].reshape(1, -1).shape)\n",
    "cosine_similarity(word_to_vec_map[\"bethesda\"].reshape(1, -1), word_to_vec_map[\"ea\"].reshape(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's load the data set and label set we processed in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_processed_data(data_set = '../data/data_set.txt', labels = '../data/labels.txt'):\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    with open(data_set) as data_file:\n",
    "        for line in data_file:\n",
    "            line = line.strip()\n",
    "            if len(line) > 0:\n",
    "                X.append(line)\n",
    "    \n",
    "    with open(labels) as label_file:\n",
    "        for line in label_file:\n",
    "            line = line.strip()\n",
    "            if len(line) > 0:\n",
    "                Y.append(line)\n",
    "                \n",
    "    X = np.asarray(X)\n",
    "    Y = np.asarray(Y, dtype=int)\n",
    "    \n",
    "    return X, Y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = read_processed_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X :  150\n",
      "Y :  150\n"
     ]
    }
   ],
   "source": [
    "print('X : ', len(X))\n",
    "print('Y : ', len(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's shuffle the data set to mix the positive and negative examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sean Murray and Hello Games this silence is deafening 1\n"
     ]
    }
   ],
   "source": [
    "X, Y = shuffle(X, Y, random_state=0)\n",
    "print(X[len(X) - 1], Y[len(Y) - 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to find the maximum word count in a sentence in the data set. This value will be used for the masking operation in creating the word indices and will be used to determine the number of time steps in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_len(X):\n",
    "    \n",
    "    max_len = 0\n",
    "    max_line = None\n",
    "    max_array = []\n",
    "    \n",
    "    \n",
    "    for line in X:\n",
    "        sentence_words =line.lower().split()\n",
    "        if len(sentence_words) > max_len:\n",
    "            max_len = len(sentence_words)\n",
    "            max_line = line \n",
    "            max_array = sentence_words\n",
    "    \n",
    "    print ('Max length is ', max_len)\n",
    "    print(max_line)\n",
    "    print(max_array)\n",
    "    \n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length is  54\n",
      "Look guys Anthem is in a bad place right now but remember to be civil towards the devs They are people too and probably under a bit of stress right now Voice your feedback but be considerate and most of all remember there is an actual living person on the other side of it\n",
      "['look', 'guys', 'anthem', 'is', 'in', 'a', 'bad', 'place', 'right', 'now', 'but', 'remember', 'to', 'be', 'civil', 'towards', 'the', 'devs', 'they', 'are', 'people', 'too', 'and', 'probably', 'under', 'a', 'bit', 'of', 'stress', 'right', 'now', 'voice', 'your', 'feedback', 'but', 'be', 'considerate', 'and', 'most', 'of', 'all', 'remember', 'there', 'is', 'an', 'actual', 'living', 'person', 'on', 'the', 'other', 'side', 'of', 'it']\n"
     ]
    }
   ],
   "source": [
    "max_len = find_max_len(X)\n",
    "Tx = max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_dev_test(X, Y, split):\n",
    "    \n",
    "    X, Y = shuffle(X, Y)\n",
    "    \n",
    "    mode = split[0]\n",
    "    \n",
    "    if mode == 'tt':\n",
    "        train_size, test_size = split[1]\n",
    "        \n",
    "        X_train = X[:train_size]\n",
    "        Y_train = Y[:train_size]\n",
    "    \n",
    "        X_test = X[train_size:]\n",
    "        Y_test = Y[train_size:]\n",
    "        \n",
    "        print('Size of test set : ', len(X_train))\n",
    "        print('Size of train set : ', len(X_test))\n",
    "    \n",
    "        result = (X_train, Y_train, X_test, Y_test)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        train_size, dev_size, test_size = split[1]\n",
    "        \n",
    "        X_train = X[:train_size]\n",
    "        Y_train = Y[:train_size]\n",
    "    \n",
    "        X_dev = X[train_size:train_size + dev_size]\n",
    "        Y_dev = Y[train_size:train_size + dev_size]\n",
    "    \n",
    "        X_test = X[train_size + dev_size:]\n",
    "        Y_test = Y[train_size + dev_size:]\n",
    "        \n",
    "        print('Size of test set : ', len(X_train))\n",
    "        print('Size of dev set : ', len(X_dev))\n",
    "        print('Size of train set : ', len(X_test))\n",
    "    \n",
    "        result = (X_train, Y_train, X_dev, Y_dev, X_test, Y_test)\n",
    "\n",
    "    \n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of test set :  100\n",
      "Size of dev set :  25\n",
      "Size of train set :  25\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_dev, Y_dev, X_test, Y_test = create_train_dev_test(X, Y, ['tvt', (100, 25, 25)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You binge in the visual arts but are scarce when it comes to storytelling 1\n"
     ]
    }
   ],
   "source": [
    "print(X_train[len(X_train) - 1], Y_train[len(Y_train) - 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of test set :  100\n",
      "Size of train set :  50\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = create_train_dev_test(X, Y, ['tt', (100, 50)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define the function to turn the sentences of the data set into word indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len, unknown_word_index):\n",
    "    \"\"\"\n",
    "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
    "    The output shape should be such that it can be given to `Embedding()` (described in Figure 4). \n",
    "    \n",
    "    Arguments:\n",
    "    X -- array of sentences (strings), of shape (m, 1)\n",
    "    word_to_index -- a dictionary containing the each word mapped to its index\n",
    "    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. \n",
    "    \n",
    "    Returns:\n",
    "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]                                   # number of training examples\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Initialize X_indices as a numpy matrix of zeros and the correct shape (≈ 1 line)\n",
    "    X_indices = np.zeros((m, max_len))\n",
    "    \n",
    "    for i in range(m):                               # loop over training examples\n",
    "        \n",
    "        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.\n",
    "        sentence_words =X[i].lower().split()\n",
    "        \n",
    "        # Initialize j to 0\n",
    "        j = 0\n",
    "        \n",
    "        # Loop over the words of sentence_words\n",
    "        for w in sentence_words:\n",
    "            # Set the (i,j)th entry of X_indices to the index of the correct word.\n",
    "            if w in word_to_index:\n",
    "                X_indices[i, j] = word_to_index[w]\n",
    "                # Increment j to j + 1\n",
    "                j = j + 1\n",
    "            else:\n",
    "                X_indices[i, j] = unknown_word_index\n",
    "                j = j + 1\n",
    "            \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to feed our pre-trained word encodings into the Keras embedding layer. This function will take care of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index, unkown_word_vector, unknown_word_index):\n",
    "    \"\"\"\n",
    "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
    "    \n",
    "    Arguments:\n",
    "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    embedding_layer -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab_len = unknown_word_index + 1                  # adding 1 to fit Keras embedding (requirement)\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors (= 50)\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "    \n",
    "    emb_matrix[unknown_word_index, :] = unkown_word_vector\n",
    "\n",
    "    # Define Keras embedding layer with the correct output/input sizes, make it non-trainable. Use Embedding(...). Make sure to set trainable=False. \n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the \"None\".\n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define custom softmax function to be used in the Attention mechanism's alpha calculation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, axis=1):\n",
    "    \"\"\"Softmax activation function.\n",
    "    # Arguments\n",
    "        x : Tensor.\n",
    "        axis: Integer, axis along which the softmax normalization is applied.\n",
    "    # Returns\n",
    "        Tensor, output of softmax transformation.\n",
    "    # Raises\n",
    "        ValueError: In case `dim(x) == 1`.\n",
    "    \"\"\"\n",
    "    ndim = K.ndim(x)\n",
    "    if ndim == 2:\n",
    "        return K.softmax(x)\n",
    "    elif ndim > 2:\n",
    "        e = K.exp(x - K.max(x, axis=axis, keepdims=True))\n",
    "        s = K.sum(e, axis=axis, keepdims=True)\n",
    "        return e / s\n",
    "    else:\n",
    "        raise ValueError('Cannot apply softmax to a tensor that is 1D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define global variables for the attention step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0818 14:48:04.824707 4613428672 deprecation_wrapper.py:119] From /Users/dulithadabare/anaconda3/envs/comment-net-nomkl/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Defined shared layers as global variables\n",
    "repeator = RepeatVector(Tx)\n",
    "concatenator = Concatenate(axis=-1)\n",
    "densor1 = Dense(10, activation = \"tanh\")\n",
    "densor2 = Dense(1, activation = \"relu\")\n",
    "activator = Activation(softmax, name='attention_weights') # We are nor using the custom softmax(axis = 1) loaded in this notebook\n",
    "dotor = Dot(axes = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function to calculate the context for the each step in the final Bi-LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_attention(a, s_prev):\n",
    "    \"\"\"\n",
    "    Performs one step of attention: Outputs a context vector computed as a dot product of the attention weights\n",
    "    \"alphas\" and the hidden states \"a\" of the Bi-LSTM.\n",
    "    \n",
    "    Arguments:\n",
    "    a -- hidden state output of the Bi-LSTM, numpy-array of shape (m, Tx, n_a)\n",
    "    s_prev -- previous hidden state of the (post-attention) LSTM, numpy-array of shape (m, n_s)\n",
    "    \n",
    "    Returns:\n",
    "    context -- context vector, input of the next (post-attetion) LSTM cell\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Use repeator to repeat s_prev to be of shape (m, Tx, n_s) so that you can concatenate it with all hidden states \"a\" (≈ 1 line)\n",
    "    s_prev = repeator(s_prev)\n",
    "    # Use concatenator to concatenate a and s_prev on the last axis (≈ 1 line)\n",
    "    concat = concatenator([a, s_prev])\n",
    "    # Use densor1 to propagate concat through a small fully-connected neural network to compute the \"intermediate energies\" variable e. (≈1 lines)\n",
    "    e = densor1(concat)\n",
    "    # Use densor2 to propagate e through a small fully-connected neural network to compute the \"energies\" variable energies. (≈1 lines)\n",
    "    energies = densor2(e)\n",
    "    # Use \"activator\" on \"energies\" to compute the attention weights \"alphas\" (≈ 1 line)\n",
    "    alphas = activator(energies)\n",
    "    # Use dotor together with \"alphas\" and \"a\" to compute the context vector to be given to the next (post-attention) LSTM-cell (≈ 1 line)\n",
    "    context = dotor([alphas, a])\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the necessary parts, let's create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CommentNet(input_shape, word_to_vec_map, word_to_index, unkown_word_vector, unknown_word_index):\n",
    "    \"\"\"\n",
    "    Function creatiDabareng the Emojify-v2 model's graph.\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the input, usually (max_len,)\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).\n",
    "    sentence_indices = Input(shape = input_shape, dtype = 'int32')\n",
    "    \n",
    "    # Create the embedding layer pretrained with GloVe Vectors (≈1 line)\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index, unkown_word_vector, unknown_word_index)\n",
    "    \n",
    "    # Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
    "    embeddings = embedding_layer(sentence_indices)   \n",
    "    \n",
    "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a batch of sequences.\n",
    "    a = Bidirectional(LSTM(128, return_sequences = True), merge_mode = 'ave')(embeddings)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    #X = Dropout(rate = 0.5)(X)\n",
    "    \n",
    "    s0 = Input(shape=(128,), name='s0')\n",
    "    c0 = Input(shape=(128,), name='c0')\n",
    "    s = s0\n",
    "    c = c0\n",
    "    \n",
    "    post_attention_Bi_LSTM_cell = LSTM(128, return_state = True)\n",
    "    \n",
    "    for t in range(1):\n",
    "        \n",
    "        context = one_step_attention(a, s)\n",
    "        \n",
    "        s, _, c = post_attention_Bi_LSTM_cell(context, initial_state = [s, c])\n",
    "        \n",
    "    \n",
    "    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n",
    "    # Be careful, the returned output should be a single hidden state, not a batch of sequences.\n",
    "    \n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(rate = 0.5)(s)\n",
    "    # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.\n",
    "    X = Dense(1, activation='sigmoid')(X)\n",
    "    \n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = Model(inputs = [sentence_indices, s0, c0], outputs = X)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0818 14:48:04.891263 4613428672 deprecation_wrapper.py:119] From /Users/dulithadabare/anaconda3/envs/comment-net-nomkl/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0818 14:48:05.660824 4613428672 deprecation_wrapper.py:119] From /Users/dulithadabare/anaconda3/envs/comment-net-nomkl/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0818 14:48:05.680500 4613428672 deprecation_wrapper.py:119] From /Users/dulithadabare/anaconda3/envs/comment-net-nomkl/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0818 14:48:05.681317 4613428672 deprecation_wrapper.py:119] From /Users/dulithadabare/anaconda3/envs/comment-net-nomkl/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "W0818 14:48:06.754379 4613428672 deprecation.py:506] From /Users/dulithadabare/anaconda3/envs/comment-net-nomkl/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "model = CommentNet((max_len,), word_to_vec_map, word_to_index, unkown_word_vector, unknown_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 54)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 54, 50)       20000100    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "s0 (InputLayer)                 (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 54, 128)      183296      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 54, 128)      0           s0[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 54, 256)      0           bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 54, 10)       2570        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 54, 1)        11          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "attention_weights (Activation)  (None, 54, 1)        0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1, 128)       0           attention_weights[0][0]          \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "c0 (InputLayer)                 (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   [(None, 128), (None, 131584      dot_1[0][0]                      \n",
      "                                                                 s0[0][0]                         \n",
      "                                                                 c0[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           lstm_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            129         dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 20,317,690\n",
      "Trainable params: 317,590\n",
      "Non-trainable params: 20,000,100\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0818 14:48:06.803622 4613428672 deprecation_wrapper.py:119] From /Users/dulithadabare/anaconda3/envs/comment-net-nomkl/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0818 14:48:06.828675 4613428672 deprecation.py:323] From /Users/dulithadabare/anaconda3/envs/comment-net-nomkl/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_indices = sentences_to_indices(X_train, word_to_index, max_len, unknown_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "No Mans Sky did to me what no shrink could ever do\n",
      "[262350. 233462. 334111. 123517. 360915. 239105. 386307. 262350. 330160.\n",
      " 110156. 141863. 126552.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.]\n"
     ]
    }
   ],
   "source": [
    "train_index = 60\n",
    "print(X_train.shape[0])\n",
    "print(X_train[train_index])\n",
    "print(X_train_indices[train_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "s0 = np.zeros((X_train.shape[0], 128))\n",
    "c0 = np.zeros((X_train.shape[0], 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "100/100 [==============================] - 3s 25ms/step - loss: 0.6864 - acc: 0.6500\n",
      "Epoch 2/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.6667 - acc: 0.7700\n",
      "Epoch 3/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.6201 - acc: 0.7700\n",
      "Epoch 4/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.5666 - acc: 0.7700\n",
      "Epoch 5/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.5668 - acc: 0.7700\n",
      "Epoch 6/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.5550 - acc: 0.7700\n",
      "Epoch 7/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.5512 - acc: 0.7700\n",
      "Epoch 8/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.5351 - acc: 0.7700\n",
      "Epoch 9/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.5412 - acc: 0.7700\n",
      "Epoch 10/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.5315 - acc: 0.7700\n",
      "Epoch 11/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.5292 - acc: 0.7700\n",
      "Epoch 12/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.5359 - acc: 0.7700\n",
      "Epoch 13/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.5203 - acc: 0.7700\n",
      "Epoch 14/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.5105 - acc: 0.7700\n",
      "Epoch 15/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.5130 - acc: 0.7800\n",
      "Epoch 16/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.5234 - acc: 0.8000\n",
      "Epoch 17/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.5040 - acc: 0.8100\n",
      "Epoch 18/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.5111 - acc: 0.7700\n",
      "Epoch 19/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.5030 - acc: 0.7800\n",
      "Epoch 20/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.4983 - acc: 0.7900\n",
      "Epoch 21/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.4938 - acc: 0.8000\n",
      "Epoch 22/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.4782 - acc: 0.8200\n",
      "Epoch 23/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.4670 - acc: 0.8300\n",
      "Epoch 24/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.4436 - acc: 0.8400\n",
      "Epoch 25/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.4910 - acc: 0.8000\n",
      "Epoch 26/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.4841 - acc: 0.8000\n",
      "Epoch 27/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.4716 - acc: 0.8000\n",
      "Epoch 28/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.4490 - acc: 0.8200\n",
      "Epoch 29/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.4375 - acc: 0.8400\n",
      "Epoch 30/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.4088 - acc: 0.8600\n",
      "Epoch 31/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.4213 - acc: 0.8500\n",
      "Epoch 32/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.4359 - acc: 0.8300\n",
      "Epoch 33/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.4027 - acc: 0.8500\n",
      "Epoch 34/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.3986 - acc: 0.8300\n",
      "Epoch 35/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.3528 - acc: 0.8700\n",
      "Epoch 36/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.3489 - acc: 0.8600\n",
      "Epoch 37/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.3416 - acc: 0.8300\n",
      "Epoch 38/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.3160 - acc: 0.8200\n",
      "Epoch 39/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.2946 - acc: 0.9000\n",
      "Epoch 40/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.2462 - acc: 0.8900\n",
      "Epoch 41/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.2511 - acc: 0.9100\n",
      "Epoch 42/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.2876 - acc: 0.9000\n",
      "Epoch 43/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.2574 - acc: 0.8900\n",
      "Epoch 44/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.1914 - acc: 0.8900\n",
      "Epoch 45/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.1636 - acc: 0.9200\n",
      "Epoch 46/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.1755 - acc: 0.9200\n",
      "Epoch 47/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.1695 - acc: 0.9300\n",
      "Epoch 48/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.2763 - acc: 0.8900\n",
      "Epoch 49/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.1510 - acc: 0.9300\n",
      "Epoch 50/50\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.1359 - acc: 0.9600\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13a910b70>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X_train_indices, s0, c0], Y_train, epochs = 50, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len, unknown_word_index)\n",
    "s0 = np.zeros((X_test_indices.shape[0], 128))\n",
    "c0 = np.zeros((X_test_indices.shape[0], 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 8ms/step\n",
      "\n",
      "Test accuracy =  0.7400000095367432\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate([X_test_indices, s0, c0], Y_test)\n",
    "print()\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : Anthem Endgame Loot Guide How to perfect your build in hours\n",
      "Expected label:0 Prediction:  1\n",
      "14 : I gotta give it up to uValseek for being around when the Crap hits the fan\n",
      "Expected label:0 Prediction:  1\n",
      "17 : More Objective Radar variations based on the original design\n",
      "Expected label:0 Prediction:  1\n",
      "20 : Less than 950 playing right now\n",
      "Expected label:1 Prediction:  0\n",
      "21 : PowerScaling Why Loot Doesnt Matter Anymore\n",
      "Expected label:0 Prediction:  1\n",
      "32 : BioWare  Can Colossus PLEASE get Shield Customization as our 4th armor Piece\n",
      "Expected label:0 Prediction:  1\n",
      "38 : Idea don’t just increase the size of the stash but make it upgradable with caps and rare resources\n",
      "Expected label:0 Prediction:  1\n",
      "39 : Argue with me all you want but Bethesda is actually trying and I can’t thank them enough\n",
      "Expected label:0 Prediction:  1\n",
      "42 : Reward structure issues and ideas\n",
      "Expected label:0 Prediction:  1\n",
      "45 : Share your support  thanks to Hello Games a company that could have just given up but instead they just keep giving\n",
      "Expected label:0 Prediction:  1\n",
      "47 : Bugs after Patch 10114\n",
      "Expected label:0 Prediction:  1\n",
      "48 : Ive compiled a list of all known bugs in Fallout 76 xpost rFallout\n",
      "Expected label:0 Prediction:  1\n",
      "49 : Mythbusters and mechanics not explicitly stated or clarified in the game anywhere\n",
      "Expected label:0 Prediction:  1\n",
      "Test count : 50\n",
      "Error count : 13\n",
      "F1 Score for Trolling:  0.8354430379746834\n",
      "Constructive feedback count :  16\n",
      "Predicted Constructive feedback count :  5\n",
      "F1 Score Constructive Feedback:  0.38095238095238093\n"
     ]
    }
   ],
   "source": [
    "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len, unknown_word_index)\n",
    "pred = model.predict([X_test_indices, s0, c0])\n",
    "\n",
    "threshold = 0.5\n",
    "Y_pred = pred > threshold\n",
    "\n",
    "\n",
    "error_count = 0;\n",
    "for i in range(len(X_test)):\n",
    "    x = X_test_indices\n",
    "    if(Y_pred[i] != Y_test[i]):\n",
    "        error_count = error_count + 1\n",
    "        print( str(i) + ' : ' + X_test[i])\n",
    "        print('Expected label:'+ str(Y_test[i]) + ' Prediction: ' + ' ' + str( 1 if Y_pred[i] else 0))\n",
    "print('Test count : ' + str(len(X_test)))\n",
    "print('Error count : ' + str(error_count))\n",
    "\n",
    "print('F1 Score for Trolling: ', f1_score(Y_test, Y_pred))\n",
    "\n",
    "Y_test_inv = np.invert(Y_test > 0).reshape(-1,1)\n",
    "Y_pred_inv = np.invert(Y_pred).reshape(-1,1)\n",
    "\n",
    "print('Constructive feedback count : ', np.sum(Y_test_inv))\n",
    "print('Predicted Constructive feedback count : ', np.sum(Y_pred_inv))\n",
    "print('F1 Score Constructive Feedback: ', f1_score(Y_test_inv, Y_pred_inv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[119292.  74527. 126848. 264550. 176551. 357266. 144588. 106074.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.]\n",
      "1 : Defending Bethesda does not help the Fallout community\n",
      "Expected label:1 Prediction:  [ True]\n",
      "[ 90548. 384374. 286531. 200035. 174642.  43010. 108628. 270193. 295922.\n",
      " 157049.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.]\n",
      "2 : Can we please just have a coop online PvE game\n",
      "Expected label:1 Prediction:  [ True]\n",
      "[ 76699.  62065. 358160. 287479. 394475.  58997.  54273. 136156. 360915.\n",
      " 357266. 157151. 389938.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.]\n",
      "3 : BioWare at this point you are an embarrassment to the Gaming world\n",
      "Expected label:1 Prediction:  [ True]\n",
      "[132265. 344148. 373317. 222041.  44608. 131001.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.]\n",
      "4 : EA straight up lied about duplicates\n",
      "Expected label:1 Prediction:  [ True]\n",
      "[119292.  65963. 279457. 218212. 313143. 357266. 155653. 268046. 279457.\n",
      " 157151.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.]\n",
      "5 : Defending bad PC launches ruins the future of PC gaming\n",
      "Expected label:1 Prediction:  [ True]\n",
      "[394475.  69897. 188481. 218763. 154323. 332120. 118605. 360915. 221115.\n",
      " 121839.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.]\n",
      "6 : You bask in laziness from simple decals to level design\n",
      "Expected label:1 Prediction:  [ True]\n",
      "[185457. 388583. 185457. 259914. 348218.  56280.  88126.  87992. 357266.\n",
      " 157049.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.]\n",
      "7 : I wish I never supported Anthem by buying the game\n",
      "Expected label:1 Prediction:  [ True]\n",
      "[185457. 209645. 357652.  43010. 225985. 163745. 269798. 308561. 264937.\n",
      "  87775. 384374. 119370. 258451. 137256. 384426. 307115. 137938.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.]\n",
      "8 : I know theres a lot going on right now but we definitely need energy weapons revamped entirely\n",
      "Expected label:0 Prediction:  [False]\n",
      "[ 90548. 384374. 343812. 400001. 141950.  54718. 237426. 193637. 272930.\n",
      " 337259. 225289.  64276.  86248.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.]\n",
      "9 : Can we stop nerfing everything and maybe issue out some long awaited buffs\n",
      "Expected label:1 Prediction:  [ True]\n",
      "[ 57187. 135869. 357970. 358160. 157049. 192973. 127853.  54718. 400001.\n",
      " 121794.  51582. 357266. 174339. 185457. 239170. 267359. 357965. 258451.\n",
      " 360915.  71090. 149340.  87775.  88126. 262350. 239197. 192973. 358160.\n",
      " 157049. 313137.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.]\n",
      "10 : Anyone else think this game is dope and doesn’t deserve all the hate I mean obviously things need to be fixed but by no means is this game ruined\n",
      "Expected label:0 Prediction:  [False]\n",
      "[ 90548. 384374. 264550. 290559. 123393.  54718. 132265. 386424. 357810.\n",
      " 189556. 149328. 358160. 386897. 108327.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.]\n",
      "11 : Can we NOT praise DICE and EA when they inevitably fix this whole controversy\n",
      "Expected label:1 Prediction:  [ True]\n",
      "[323393.  43010. 225985. 268046. 289786.  44608. 286421. 105730.  65680.\n",
      " 360915. 357266. 157049.  54718. 264937. 226371. 193716. 299527. 394565.\n",
      " 172466. 186645. 394475. 259914. 219577.  71494. 400001.  52879. 226298.\n",
      " 357266. 157049.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.]\n",
      "12 : Seeing a lot of posts about players coming back to the game and now loving it Raise your hand if you never left because youve always loved the game\n",
      "Expected label:0 Prediction:  [False]\n",
      "[ 56280. 192973.  54273.  45919. 319985.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.]\n",
      "13 : Anthem is an actual scam\n",
      "Expected label:1 Prediction:  [ True]\n",
      "[400001. 153443. 360915. 286375.  58997. 394475. 120978.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.]\n",
      "15 : FO76 free to play Are you dense\n",
      "Expected label:1 Prediction:  [ True]\n",
      "[394475. 108153. 360915. 190879. 374021.  88126. 293484. 221998.  47798.\n",
      " 221998. 111453.  54273.  63245. 268046. 125845.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.]\n",
      "16 : You continue to insult us by propagating lie after lie creating an aura of distrust\n",
      "Expected label:1 Prediction:  [ True]\n",
      "[ 41112. 117967. 268046. 331640. 154323. 176468. 157111.  44608. 357354.\n",
      " 157049. 322741. 253355. 386112. 105625. 360915.  68890.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.]\n",
      "18 : 90 Days of silence from Hello Games about their game Sean Murray weve come to bargain\n",
      "Expected label:1 Prediction:  [ True]\n",
      "[220930.  71090. 281283. 102471. 177231. 132265. 192973.   5382.  62065.\n",
      " 145731. 151349. 357266. 341956. 268046. 357266. 157049. 264550. 157109.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.]\n",
      "19 : Lets be perfectly clear here EA is 100 at fault for the state of the game not gamers\n",
      "Expected label:1 Prediction:  [ True]\n",
      "[394475. 259914. 142852. 357266. 239295. 268046. 394565. 157049. 359056.\n",
      " 384374.  58997. 151413. 360915. 339116.  54718.  58997. 219577. 130647.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.]\n",
      "22 : You never explain the mechanics of your game thus we are forced to speculate and are left dumbfounded\n",
      "Expected label:1 Prediction:  [ True]\n",
      "[169754. 357266. 231068. 292794. 177231. 192973. 357212. 293190. 192973.\n",
      " 359469. 360915. 309704. 394475. 132451. 264612. 151349.  43010. 102208.\n",
      "  88126.  45929. 286442. 357212. 102208.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.]\n",
      "23 : Guys the MAIN PROBLEM here is that progression is tied to RNG You earn nothing for a class by actually playing that class\n",
      "Expected label:1 Prediction:  [ True]\n",
      "[394475. 136929. 108628. 286375.  87775. 151407.  54273. 138116. 386474.\n",
      " 192973. 122561. 268046.  57170. 239197. 268046. 106043.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.]\n",
      "24 : You encourage coop play but force an environment which is devoid of any means of communication\n",
      "Expected label:1 Prediction:  [ True]\n",
      "[394475. 307133. 394565.  60105. 188481.  43010. 215502. 268046.  57598.\n",
      " 270970.  46701. 268046. 357266. 157111. 155243. 193642.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.]\n",
      "25 : You reveal your arrogance in a lack of apology or admission of the games fundamental issues\n",
      "Expected label:1 Prediction:  [ True]\n",
      "[223704. 141960. 357810. 319691. 357810. 149340. 192973.  84723.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.]\n",
      "26 : Literally eveything they say they fixed is broken\n",
      "Expected label:1 Prediction:  [ True]\n",
      "[270501. 220941. 360915.  74527. 157049. 345181. 394475.  58997.  83369.\n",
      " 357266. 164431. 268046. 394565. 286410.  69709. 145491. 357109. 394475.\n",
      "  58997. 149363. 193716.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.]\n",
      "27 : Open letter to Bethesda Game Studios You are breaking the goodwill of your player base faster than you are fixing it\n",
      "Expected label:1 Prediction:  [ True]\n",
      "[137680. 388711. 357266. 144617.  47057.  74527.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.]\n",
      "28 : Enough with the false advertising Bethesda\n",
      "Expected label:1 Prediction:  [ True]\n",
      "[188481. 358160. 157049. 185457. 400001. 146662. 319039. 388711.  57170.\n",
      " 268046. 254258. 388345. 393705. 126848.  57178. 135869. 383068. 358160.\n",
      " 190756.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.]\n",
      "29 : In this game I haven’t felt satisfied with any of my wins yet Does anybody else want this instead\n",
      "Expected label:1 Prediction:  [ True]\n",
      "[394475. 102496. 174642. 262350. 188748. 270970. 129440. 360915. 188386.\n",
      " 358160. 157049.  88126. 125973. 306496. 360915. 272583. 293244.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.]\n",
      "30 : You clearly have no incentive or drive to improve this game by diverging resources to other projects\n",
      "Expected label:1 Prediction:  [ True]\n",
      "[360915.  76699. 142492. 218973. 394475. 343645.  43010. 386897. 225985.\n",
      " 268046. 248489. 154323. 374021.  54718. 384374. 121794.  43010. 302292.\n",
      "  54718.  90735. 306549.  44608. 182540. 394568. 163745. 360915. 367635.\n",
      " 358160. 329046.  59937.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.]\n",
      "31 : To BioWare Executive Leadership You stole a whole lot of money from us and we deserve a real and candid response about how youre going to turn this ship around\n",
      "Expected label:1 Prediction:  [ True]\n",
      "[400001. 337914.  87775. 358160. 157049. 258481.  43010. 337083. 247377.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.]\n",
      "33 : I’m sorry but this game needs a solo mode\n",
      "Expected label:1 Prediction:  [ True]\n",
      "[118309. 322741. 253355.  54718. 176468. 157111.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.]\n",
      "34 : Dear Sean Murray and Hello Games\n",
      "Expected label:1 Prediction:  [ True]\n",
      "[132265. 192973. 357266. 390080. 357959. 360915. 173061. 360915. 341678.\n",
      " 383443.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.]\n",
      "35 : EA is the worst thing to happen to Star Wars\n",
      "Expected label:1 Prediction:  [ True]\n",
      "[132265. 192973. 222484. 366274. 360915. 374163. 357266.  52953. 360915.\n",
      " 150079. 357266. 256745. 127708. 286375. 191811. 357354. 286836.  71090.\n",
      "  89956.  54718. 306507. 270970. 357266. 270201. 260141. 387696.  71090.\n",
      "  44608. 182540. 111389.  54718. 372713. 384374.  58997.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.]\n",
      "36 : EA is likely trying to use the AMA to flip the narrative dont play into their ploy be calm and respectful or the only news will be about how crazy and unreasonable we are\n",
      "Expected label:1 Prediction:  [ True]\n",
      "[357266. 292794. 185457. 174642. 388711. 357266. 362050. 359515. 225709.\n",
      " 188481.  56280. 192973. 264550. 357266. 129579. 300964. 357266. 302292.\n",
      " 292794. 192973. 357212. 386424. 193716. 148356. 129606. 193716. 146354.\n",
      " 264612. 338995.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.]\n",
      "37 : The problem I have with the top tier loot in anthem is not the drop rate The real problem is that when it finally drops it feels nothing special\n",
      "Expected label:0 Prediction:  [False]\n",
      "[358160. 345534. 192973. 374177.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.]\n",
      "40 : This sub is useless\n",
      "Expected label:1 Prediction:  [ True]\n",
      "[332240. 358160. 368118.  88083. 174032. 341886. 106042. 141816. 249714.\n",
      " 189895.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.]\n",
      "41 : Since this tweet BW has started communicating even more infrequent\n",
      "Expected label:1 Prediction:  [ True]\n",
      "[360915. 292042. 132265. 154323. 400001. 297580. 188481. 357266. 373370.\n",
      "  52953. 357266. 247519. 268046. 358160. 400001. 329916. 111448.  43010.\n",
      " 358522. 151349. 386307. 297580. 384374. 383068.  56162. 289455. 357212.\n",
      " 223628. 386424. 357266.  52953. 163634. 373317. 357412. 119969.  57170.\n",
      " 272583. 105851. 357226. 264550. 193716. 151428. 132265. 360915. 134678.\n",
      " 125876. 357266.  52953. 270970.  56160. 357266. 297580.      0.      0.]\n",
      "43 : To prevent EA from astroturfingplanting questions in the upcoming AMA the mods of this subreddit should create a thread for what questions we want answered post that list when the AMA goes up then delete any other comment thats not it forcing EA to either ditch the AMA or answer the questions\n",
      "Expected label:1 Prediction:  [ True]\n",
      "[394475. 305007.  43010. 157049. 188481. 155017. 209662. 357212. 193919.\n",
      "  43010. 162852. 354594. 120523.  62065. 357266. 390080.  54718.  54273.\n",
      " 371757. 157049.  62065. 193919.  74390.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.]\n",
      "44 : You released a game in full knowledge that its a glorified tech demo at the worst and an unfinished game at its best\n",
      "Expected label:1 Prediction:  [ True]\n",
      "[207715. 268046. 225979. 254258. 355512. 217787. 261149.  47798. 357266.\n",
      " 373375.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.]\n",
      "46 : Kind of lost my temper last night after the update\n",
      "Expected label:1 Prediction:  [ True]\n",
      "Test count : 50\n",
      "Error count : 37\n"
     ]
    }
   ],
   "source": [
    "correct_count = 0;\n",
    "for i in range(len(X_test)):\n",
    "    x = X_test_indices\n",
    "    if(Y_pred[i] == Y_test[i]):\n",
    "        print(X_test_indices[i])\n",
    "        correct_count = correct_count + 1\n",
    "        print( str(i) + ' : ' + X_test[i])\n",
    "        print('Expected label:'+ str(Y_test[i]) + ' Prediction: ' + ' ' + str(Y_pred[i]))\n",
    "print('Test count : ' + str(len(X_test)))\n",
    "print('Error count : ' + str(correct_count))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function to visualize the attention weights at the last time-step of the second LSTM layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_map(model, X_test_indices, X_test, n_s = 128, num = 6, Tx = 30):\n",
    "    \"\"\"\n",
    "    Plot the attention map.\n",
    "  \n",
    "    \"\"\"\n",
    "    attention_map = np.zeros((1, Tx))\n",
    "    Ty, Tx = attention_map.shape\n",
    "    \n",
    "    s0 = np.zeros((1, n_s))\n",
    "    c0 = np.zeros((1, n_s))\n",
    "    layer = model.layers[num]\n",
    "    print(layer.name)\n",
    "\n",
    "    f = K.function(model.inputs, [layer.get_output_at(t) for t in range(1)])\n",
    "    r = f([X_test_indices, s0, c0])\n",
    "    \n",
    "    for t in range(1):\n",
    "        for t_prime in range(Tx):\n",
    "            attention_map[t][t_prime] = r[t][0,t_prime,0]\n",
    "            \n",
    "    \n",
    "    print(attention_map)\n",
    "\n",
    "    # Normalize attention map\n",
    "#     row_max = attention_map.max(axis=1)\n",
    "#     attention_map = attention_map / row_max[:, None]\n",
    "\n",
    "    prediction = model.predict([X_test_indices, s0, c0])\n",
    "    \n",
    "    Y_prediction = prediction > threshold\n",
    "    predicted_text = []\n",
    "   # for i in range(len(prediction)):\n",
    "  #      predicted_text.append(int(np.argmax(prediction[i], axis=1)))\n",
    "        \n",
    "    #predicted_text = list(predicted_text)\n",
    "   # predicted_text = int_to_string(predicted_text, inv_output_vocabulary)\n",
    "    text_ = X_test.lower().split()\n",
    "    \n",
    "    # get the lengths of the string\n",
    "    input_length = len(text_)\n",
    "    output_length = Tx\n",
    "    \n",
    "    # Plot the attention_map\n",
    "    plt.clf()\n",
    "    f = plt.figure(figsize=(25, 25.5))\n",
    "    ax = f.add_subplot(1, 1, 1)\n",
    "\n",
    "    # add image\n",
    "    i = ax.imshow(attention_map, interpolation='nearest', cmap='Blues')\n",
    "\n",
    "    # add colorbar\n",
    "    cbaxes = f.add_axes([0.2, 0, 0.6, 0.03])\n",
    "    cbar = f.colorbar(i, cax=cbaxes, orientation='horizontal')\n",
    "    cbar.ax.set_xlabel('Alpha value (Probability output of the \"softmax\")', labelpad=2)\n",
    "\n",
    "    # add labels\n",
    "  #  ax.set_yticks(range(output_length))\n",
    "  #  ax.set_yticklabels(predicted_text[:output_length])\n",
    "\n",
    "    ax.set_xticks(range(input_length))\n",
    "    ax.set_xticklabels(text_[:input_length], rotation='60')\n",
    "\n",
    "    ax.set_xlabel('Input Sequence')\n",
    "    ax.set_ylabel('Output Sequence')\n",
    "\n",
    "    # add grid and legend\n",
    "    ax.grid()\n",
    "\n",
    "    #f.show()\n",
    "    \n",
    "    return attention_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_weights\n",
      "[[0.00769839 0.01270912 0.022584   0.03538825 0.03264895 0.02746085\n",
      "  0.02200335 0.02269096 0.02587707 0.02616745 0.02334025 0.02811753\n",
      "  0.0291469  0.03028349 0.02689946 0.02245964 0.02875153 0.03037322\n",
      "  0.02669312 0.0324071  0.02416801 0.0212603  0.02024181 0.01361241\n",
      "  0.01350428 0.01346731 0.01343916 0.01342383 0.01341562 0.01341205\n",
      "  0.01341172 0.0134139  0.0134182  0.01342445 0.01343256 0.01344251\n",
      "  0.01345432 0.01346803 0.01348369 0.01350133 0.013521   0.01354271\n",
      "  0.01356648 0.01359231 0.01362018 0.0136501  0.013682   0.0137158\n",
      "  0.01375123 0.01378775 0.01382428 0.01385869 0.01388732 0.013904  ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABa4AAAPxCAYAAAASazoTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzdd7hcVdWA8XelkIQQmvQO0kFpAUREUIpYAAuISJciCAgiCogoiigqIqIiYC8oTUVAinQQQQgISu8ooUkPIT3r+2OfgTFfyszcmTv35r6/57lPppw5a8/MmTOZddZZOzITSZIkSZIkSZL6ikHdHoAkSZIkSZIkSfVMXEuSJEmSJEmS+hQT15IkSZIkSZKkPsXEtSRJkiRJkiSpTzFxLUmSJEmSJEnqU0xcS5IkSZIkSZL6FBPXkiRJkiRJkqQ+paHEdUQsHxFbVZdHRMSozg5LkiRJkiRJkjRQzTFxHRH7AecDZ1Q3LQNc0MlBSZIkSZIkSZIGrkYqrg8CNgVeAcjMB4HFOjkoSZIkSZIkSdLA1UjielJmTq5diYghQHZuSJIkSZIkSZKkgayRxPV1EfEFYEREbA2cB1zU2WFJkiRJkiRJkgaqyJx98XREDAL2AbYBArgc+EnO6YGSJEmSJEmSJLWgkcT1SGBiZk6rrg8GhmXma70wPkmSJEmSJEnSANNIq5CrgBF110cAV3ZmOJIkSZIkSZKkga6RxPXwzHy1dqW6PG/nhiRJkiRJkiRJGsgaSVyPj4j1a1ciYgNgQueGJEmSJEmSJEkayIY0sMxhwHkR8SRlcsYlgJ07OipJkiRJkiRJ0oA1x8kZASJiKLBadfX+zJzS0VFJkiRJkiRJkgasRhPXbwdWoK5COzN/1blhSZIkSZIkSZIGqjm2ComIXwNvBu4AplU3J2DiWpIkSZIkSZLUdnOsuI6Ie4E1s5HSbEmSJEmSJEmSemhQA8vcRZmQUZIkSZIkSZKkjptjqxBgEeCeiLgFmFS7MTO379ioJEmSJEmSJEkDViOJ6+M6PQhJkiRJkiRJkmrm2OMaICKWB1bJzCsjYl5gcGaO6/joJEmSJEmSJEkDzhx7XEfEfsD5wBnVTUsDF3RyUJIkSZIkSZKkgauRyRkPAjYFXgHIzAeBxTo5KEmSJEmSJEnSwNVI4npSZk6uXYmIIcCc+4tIkiRJkiRJktSCRhLX10XEF4AREbE1cB5wUWeHJUmSJEmSJEkaqOY4OWNEDAL2AbYBArgc+Ek2MqujJEmSJEmSJElNmmPiWpIkSZIkSZKk3jRkTgtExKPMpKd1Zq7UkRFJkiRJkiRJkga0OSaugdF1l4cDOwEL9yRoRCwMnAOsADwGfDQzX5zJctOAf1VX/52Z2/ckriRJkiRJkiSp72upVUhE3JaZG7QcNOJbwAuZeWJEHAUslJlHzmS5VzNzvlbjSJIkSZIkSZL6n0Zahaxfd3UQpQK7kUrt2dkB2KK6/EvgWuD/Ja4lSZIkSZIkSQPPHCuuI+KauqtTKa09TsrM+1sOGvFSZi5YXQ7gxdr1GZabCtxRxT0xMy+Yxfr2B/YHGD5ixAbLLbtsU+OZPn06gwYNauoxrcxpOT2nMyiaizNl+vSm4wwimU40vPzUKVObjgEwdMggpkxtYnxNPvfX4wwOpkxr/AWPQY0/9/+JEzClifd1cJPbDMAgpjOdZh/X/MbW7DYAML2FbXpIJFOziTgtzgU7ZFAydXrjcaK1TYDBkUxr4vlMb3Fy2yEBU5t5aCvvzSBo5uPZqlbiRAtvULPbQAsfz/K4Jj8705rYN9Xrq+9Pb7w3UL53m9XsdwHA4MHNbwjN7gdaec2g+W2txd1a03EGt/gdKkmSJEmteOjBB57LzEVndl9LrUIaERFXAkvM5K5jgF/WJ6oj4sXMXGgm61g6M8dGxErA1cCWmfnw7OJusMHovPHvY5oa601/vZZN3rFFU4956OlXm1oe4Il7b2GZNTZq6jHfvv6RpuO8a/hYrpm4dMPLn/vtM5uOAXDC/htzzJl/b/wBS67aWpwPL8Exf3i64eVXWm/1luJ8aq2JnHb38IaX33TdpZqOsemQJ7hx6jJNPWbi5GlNx3n3iCe5ekJz43to7MtNx9lj+Vf41ePzN7z89Fay48BeK4zjF4+Nanj55ZZofNl62456msvGzWy3NXP/uPuZluIcus4Uvnfn0IaXn/DapKZjfH5j+FYTH0+Aia9NbDrOsZsP4/jrmhvfMsvP9PtotvZf7TXOvH/ehpdfb7XmYwBsPmws101qfP959c2PtxTniNHTOWlM40nV8S+PbynOsVsM4/hrG39/lltpsaZj7LfKeH784MimHnPvLfc2HeeEDy7GMRc829Rjttxuw6bjfPBNz3HB84s0vPzKLe5vNuBxbmP5hpdfcHhrJ7utNukR7h/W+Hza26+2eEtxmtXqAUZJkvqqDqVW1Aa+NX1bp/KS6j82XGnB2zJz9Mzua6RVyOGzuz8zT57F7VvNZp3PRMSSmflURCwJzPSXaGaOrf59JCKuBdYDZpu4liRJkiRJkiT1b42Ue40GDgSWrv4OANYHRlV/rbgQ2LO6vCfwpxkXiIiFImJYdXkRYFPgnhbjSZIkSZIkSZL6iUbOO10GWD8zxwFExHHAnzNztx7EPRE4NyL2AR4HPlqtezRwQGbuC6wBnBER0ykJ9hMz08S1JEmSJEmSJM3lGklcLw5Mrrs+ubqtZZn5PLDlTG4fA+xbXf4b8JaexJEkSZIkSZIk9T+NJK5/BdwSEX+srn8Q+GXnhiRJkiRJkiRJGsjmmLjOzBMi4lJgs+qmvTPzH50dliRJkiRJkiRpoGpkckaAeYFXMvN7wBMRsWIHxyRJkiRJkiRJGsDmmLiOiC8DRwJHVzcNBX7TyUFJkiRJkiRJkgauRiquPwRsD4wHyMwngVHtCB4R20bE/RHxUEQcNZP7h0XEOdX9f4+IFdoRV5IkSZIkSZLUdzWSuJ6cmQkkQESMbEfgiBgM/BB4L7AmsEtErDnDYvsAL2bmysB3gW+2I7YkSZIkSZIkqe9qJHF9bkScASwYEfsBVwI/bkPsjYCHMvORzJwMnA3sMMMyOwC/rC6fD2wZEdGG2JIkSZIkSZKkPipKMfUcForYGtimuvqXzLyix4EjdgS2zcx9q+u7Axtn5sF1y9xVLfNEdf3hapnnZljX/sD+AIsvvvgGvzv77KbG8uqrrzLffPM19ZhJU6Y3tTzA5InjmWd4cwXrT4+b1HScUYOmMG760IaXf+GZ5+a80EwsvchIxj43vvEHDB3WWpwFhzL2pSkNLz9s3uEtxVlsePLsxMaPi8w3Yp6mY8wXk3k1m3tcI5/RGTW7DQBMnDKt6Thvmmcaz08e3PgDmn8qJc6waTw/qfE48wxtdN7Z/7XA4Km8PG1Iw8u/NmFqS3EWnzd55rXGt7Xp05vf3ywxEp5u4uMJkNObf4OWHBU8Na65xw0d1vhrXLPosOn8d1Lj7+u8LcSA5j8748ZPbinO4iPhmSben+nTmt8GoPn3Z54WXrdFhk3nuSbeG4AJ4yc2HWfpBYcw9qXmPnPzL9j8SWILDp7KS03sB4a1uL8ZyWTG0/j3wZAWj9sPy0lMisa/fxcY3tx3R6usQpAkzW1a/KkjyU/PgPf+92x1W2aOntl9Df0yy8wrIuJ24J3AC+0cXDtk5pnAmQAbbDA6N3nHFk09/qa/Xkuzj3no6VebWh7giXtvYZk1NmrqMb+7/pGm47xr+Fiumbh0w8ufe+afm44BcML+G3PMmX9v/AFLrtpanA8vwTF/eLrh5Vdab/WW4nxqrYmcdnfjSe9N112q6RibDnmCG6cu09RjJk5uPqH87hFPcvWE5sb30NiXm46zx/Kv8KvH5294+ektJEYB9lphHL94rPHW+sst0Vob/m1HPc1l45ZoePl/3P1MS3EOXWcK37uz8eTQhNeaP4D1+Y3hW018PAEmthDn2M2Hcfx1zT1umeUb32Zq9l/tNc68f96Gl19vtUWbjgGw+bCxXDep8f3n1WMebynOEaOnc9KYxhOe41+e0FKcY7cYxvHXNv7+LLfSAk3H2G+V8fz4weYSxPfe8u+m45zwwcU45oJnm3rMlttt2HScD77pOS54fpGGl1+5xf3NBjzObSzf8PILtngwZrVJj3D/sJUaXn77VRdvKU6zPH9OkjS3aaHeSL3Et6Zva6VYTwPHLH81R8TFEbF2dXlJ4C7gE8CvI+KwNsQeCyxbd32Z6raZLhMRQ4AFgOfbEFuSJEmSJEmS1EfNrtxrxcy8q7q8N3BFZm4HbExJYPfUrcAqEbFiRMwDfAy4cIZlLgT2rC7vCFydHoqRJEmSJEmSpLna7BLX9U2FtwQuAcjMcUBrzTbrZOZU4GDgcuBe4NzMvDsivhoR21eL/RR4U0Q8BBwOHNXTuJIkSZIkSZKkvm12DRP/ExGHAE8A6wOXAUTECKAtM/dk5iVUCfG6275Ud3kisFM7YkmSJEmSJEmS+ofZVVzvA6wF7AXsnJkvVbe/Dfh5h8clSZIkSZIkSRqgZllxnZnPAgfM5PZrgGs6OShJkiRJkiRJ0sA1u4prSZIkSZIkSZJ6nYlrSZIkSZIkSVKfMsfEdURs2shtrYiIbSPi/oh4KCKOmsn9e0XEfyPijupv33bElSRJkiRJkiT1XY1UXH+/wduaEhGDgR8C7wXWBHaJiDVnsug5mblu9feTnsaVJEmSJEmSJPVts5ycMSI2Ad4OLBoRh9fdNT8wuA2xNwIeysxHqnhnAzsA97Rh3ZIkSZIkSZKkfioyc+Z3RGwObAEcAJxed9c44KLMfLBHgSN2BLbNzH2r67sDG2fmwXXL7AV8A/gv8ADwmcz8z0zWtT+wf3V1NeD+JoezCPBcs8+hBcYxztz0XIzTt+PMTc/FOH07ztz0XIwjSZIkSb1r+cxcdGZ3zDJx/foCEctn5uPtHlGDies3Aa9m5qSI+CSwc2a+uwNjGZOZo9u9XuMYpxsxjGOc3ophHOP0Vgzj9P04kiRJktRus2wVUucXEfH/stttSCCPBZatu75MdVt9jOfrrv4E+FYPY0qSJEmSJEmS+rhGEtdH1F0eDnwEmNqG2LcCq0TEipSE9ceAj9cvEBFLZuZT1dXtgXvbEFeSJEmSJEmS1IfNMXGdmbfNcNONEXFLTwNn5tSIOBi4nDLZ488y8+6I+CowJjMvBD4dEdtTEuUvAHv1NO4snNmh9RrHON2IYRzj9FYM4xint2IYp+/HkSRJkqS2aqTH9cJ1VwcBGwCnZuZqnRyYJEmSJEmSJGlgaqRVyG1AAkGpfH4U2KeTg5IkSZIkSZIkDVxzrLiWJEmSJEmSJKk3zbHiOiKGA58C3kGpvL4BOD0zJ3Z4bFKviYhBmTm92+Not4iI9OhUy3z91G61bWpu3edIkiRJktQugxpY5lfAWsD3gR9Ul3/dyUH1toho5HXoVOzBEbFgt+K3U0RE/b/9QUTsAVBLIPWnsc9KRAyu/jXp2nMfiIg3dzJA7f3qtE5s2xGxWLvX2UDMXvuMRsTIDqx2FJR9TkQM6q3n0819W0RsEhFv6cB62/6d041turfMMGeJJEmSJPV5jSRs187MfTLzmupvP0ryem6yY0RsHRGjuhD7YOCMiNg4IubtyYoiYrWIWCcihnQpSTEcoD8kS6NYDvh0RFwTEe+FMvZuHsioiYiVImLtiGhqEtSImAeYr7r6u4hYpgNji7rLS3Ty9ermNh0R6wOfBPaIiPdExPxtWGftoMLI2nubmdOq2zr6/Oo/lz2JFRHDI6K2jf24txN9nd6/RMSiEbFAROwHfLEDIU6OiCciYqPMnN7JfU51YHQZ6Pp++Z3AKRHx+TZ/z7blO6e3tune3HfWxVk6IhaJiM2BQzsdT5IkSZLaqZHJGW+PiLdl5s0AEbExMKazw+o9EbEQ8C5gOrByRNwE/LMXT+H+GbAw8FXgrog4DXislsxqVETsCnwYWBX4N/CTiLg8M19r94BnEntJ4AjgtYgYBnwlM8f3YH21U+lXA8YBUzPz2TYN93WZ+W9gdETsD3w1IvYEvpSZD1Tj6NVT+SNicGZOq8axPTAFGB8RBwLTGtwm3gJ8LCIWBRbJzCfq1t+uCuxBwLSIOBpYJjMPqh9/G9ZPtb6ubdMAmXl7ROwNnFqN4+KI+BNl/9D0GGZ4fc4HskqOH5OZP213+4i67Wk0ZR83DXgJ+FNmPt+DVa8JnFMl3c7PzGdrCbh2f15qr0dELAWsD2wI/CUzb2xnnPp4wFaU1ljbAEe3O0Zm7hsRnwOujohzgM9l5gu1+G18/z8NbAuMjYhngN9l5t3tWHeT4xgMnAw8AOwHbBIR52Xmb3uwzrZ+59B723Sv7DtrImIIsDawGbAjcNoM93tWjiRJkqQ+rZFqnw2Av0XEYxHxGHATsGFE/Csi/tnR0fWCzHwROAS4DliDUgH9sYhYqdOxI2KezBwH3AFMALYDfg7s28IpvQcBBwB/olShfQE4OyLe1sYhz8p3gSeABYDlMnN8RCwVEUObXVGVuMmIWBc4D/g9cHhEfLhKXrXLKRFxUUTMm5lnUhI8jwO/j4gTImJEb/efrZKM8wBHAnsD8wD3ZeZk4C3RQEuZzLwN+AfwAWBCRIyOiDdVdy8TEYu0aZwLAjsDXweoEnFHRsSGPV1/na5t01XCB2AjYBhwBrBMNZ69I2KtaL7Fx/YR8dYqIT8lM99HeZ+PjIgxEbF5O7e56n0aVI194ervzcAhPakez8zbgd2ASZR91f5V5fD0iHhvtLElRN3r8T1gHWBdYHeAKmHZVlW88yhJ/iHAKhGxba0CNyI+FD04M6b6fAM8SNnXrws8GhFfqYvfYxExAvgQ8EPgj5T36nMRcVR1sLbXZOa0zJwC7ArcB4wF3hsRP68OhLeibd851Rh7ZZvuxX1nLd5USqHBgpTP/wIR8ZG6ivItWvi/hiRJkiT1mkYS19sCKwKbV38rVrd9gJJo7beinP6/dWZOzcxzgW8AQymVXIt3On5mTo6IBYATgH0zc1XKj9lPA1dFxDsbWU9EbAncTEl0fiAzt6RUtq1N+WHfMVH6/86Xmd+lJGF+WN21I6U6sil1iZsDgC8DHwReoGxzn4jS0qUdLRWOAyYCT0TEUZn5fGYeSUmurAbc0InEWANWAS4GlgQWy8xvV7d/HVh9dg+svS5VJeOO1Xq+AxxVJV6uALbuyeDqXvt1gdsoib1vUir63gS8uyfrr4vTtW0aXk/4ABxImYz2B5m5O3AL5b3YDWh4O6ySQx+mTHS7PvC3Ks6l1ef+AkqLgna3C/k4cHdmHk3Zz/wJeBuwaQ/XG5Rq5A8An42IO6O02zmJcvZK21TbwsjMPAFYCvhRddfHImLZNsapvfaDgN8A7wEGUxLlu0fEScBxPan4r/b5q1LmjDg2MzegHBz5aEQ8G6WdQzscAdyQmX/OzEsoz+cCyoGLjh+UnVFE7AgskZmHUw5AfZvyHfvTartpZl1t/c6pXzUd3KZ7a99ZF69WMf48cCLlLJ6XqjifiIgvAqcAL7YzriRJkiS1UyOJ669l5uP1f/W3dXqAnVL9iFwDOCIiToqIN2fmU5Qfcvdm5k29NJQlgSeBpSNiSGZeBryPkjBpSGZeBRwDLE85LXwwpc/x3zLz8g6Mud7zVcwLgHsy84bqNO5DKO0dGlKfsKsqat8OPJCZz2Tmt4BfAEsDi7ahn+mQzHwxM3eiJMQ/GhEPR8QHMvOfmbkj8PHMnNSTOC16BJgXuIFS+UlE7AwMrbXrmZm6SvWFo7RY+W9mngHsRPmcHwFckZm/68ngqhiLA69QEv/HAP/JzO2Bu4D1erL+ujjd3KbrJ2y9nbJ9LFeN6zTgEmBMZk5tNNFctYLYG7ickrDbKiK2j4jlq/u/BqzegdP2HwXWjIi1M3NCZt5SjaGhg2L1aq9JROxO+XyfCOxLOcj0R8rZKt/vQDuKscAVUdorXJuZd1bvx6GU7bAt6l777wLrZOYD1ftyHGVCxQnA/m0INQL4a2a+VMW9HziMUhnbk3YXwOvv05rAFyPiyCrGo8ClwNerszJ628PAKxExMjNfycx/Utrl/AG4rMl1teU7B3p3m+6tfeeMIuLrwB7V98cPgR9T/n+xFOX/crYKkSRJktRnxZx+s0TE7Zm5ft31IZQer2t2enCdViWdRgPvp1QhPkCpfjspM8+PXupxHBHHUtoR/Jzyo/z9wLsyc98m1zOYUmG7CqUv8KFVtV1HVVVpx1EqZM+jVKk+mplfbPQ1jIgVMvOx6vImlJ7fiwKnZOYv6pZrSy/QqlXCupl5fXV9L+B44Clgp24clImINTLz3ihtao6itCtYiVJ1/KUqQTPb5x8Rv6X0xl4OuAe4MDMvrz63UZ2y38rYhlJ6//6T8jr9kJIkmjczH4/SU/t6YJfMvKOVGLOI26vbdMT/9nyt3ovDKC0OJlOSjrtmZsPtSuKNXtMLV49/jZLEXq9a7y3ArbVEZrtVlZVLU1rhXAz8FvhsZl7R4vr+BuxASRy/k9LG5TbgLMqBhXa1u6j1up+/Wve2wPsy84oovaHvzMyvtznWVsCJmTm6aqdwJPBcZn6jHXGqWCOAsyn9+/fOzCkRcQLwfGae3Ib1L0vZxjagVKe/ChyWmdf0dN09GNNgSpX5SpTnfhZwJXBGttDruh3fOTOsr2PbdDf2nXXb81qU1+cdwMuUA5gPAn80YS1JkiSpP5hl4rqqbvsCbyRaapWFk4Ezq1PP+6WIWIIyWdmylFP2X6VUuX4IeDBLz+PeGMdiwEjKachfolSUAqwAHJCZ/2hhnbU+tvN1MlFRJUI/TKmGfJrSxuItwJaUSrUfVT+cG5r8KSL2oBw4eDYzH6lu+zhvTFL4u8y8pNH1zSZO7Qf9e4BfVmP9dl3M04FLMvPCVmM0OZ7a5HMbAX+lJBY/A0ylVMQtD9yWmY/O6rnXPaf3AZ/OzG0j4mHgXMoBmbuB0zLznhbHGJT9wA7AFymfldVrFelV1eLHgVGZ+aNZrqhFvbVNV7Fqr+UWlG36QWAh4K2UPrEjgF9k5o3NHkSJiN8BN2XmqdX1dSmv20rAQZn5TBvGX0uSr0xJXN5L6dX8ZkqLgMWBqzLzh7NZzezWvyXlwNp3gD9n5roRsR6lBcm+mfmXNj6HIZT3/KVqG9iFUg37NGU/sWdPY80k9ucp1agXUpKhkyn7tVMz86IW1/n657ZK4I6k9FL+PuXsmiurGO/PzKd7OP5dgY9SDvTcBPwa2ITSJuagTnw+ZzGO2n5tEcoBp6UoZ5OsQDkQNIlyRs1BTayzrd85devt2DbdB/adn6S0VvoJpQ3KypT342vZw7NvJEmSJKk3NFJx/Y3+nKSemYi4jlLluBLwDHB/Zn5vhmU6Um1dl5T5KKWX5maUROlBVSJrHDCtVn3cV0XEyZRqdSjJ/1sop/G/ULdMK5VvF1GStXtl5u3VQYZPAMtm5oHtGf3rsRajVHZvSklSnJCZE9oZo4mxnEmpwtuI0qf1TMpp/Q23K6kq96+oHr9GZn4qIn4MJPCZzGypDUFELF9VBi5ISay/SEk8XZaZP4nSC/yTlMRRSxXdfUFdsm0zSsuAaymtIVar365bXPd7gK9m5sZVMuv11hRR2hQ93LPR/79411LaEQwCrqJUj94ATO5ppWVVMbwepdXB9pTE6AGZuVtP1juTON+mVL8+QEnu3ko5kDB/lr697YpTe9/XprRu+iYluXxYZl4aEd+lVPOe2sP1H0qpGn+aUmn/oyrOCOCVzHyuDc+lvnJ4c8qZPH+nHMQa09v7t4j4DeX5vhW4OTO/VN0+EnityQRzR75zqsd1ZJvu1r6z7gDcaEpv8xeB72bmzyLiCGBYlp7xkiRJktSnDWlgmUtjJpMEZtViob+JiA2BKZn5uer6xsCPqoTy66dpdyJpXa23VqF5CPAR4HBK4gLgpb6esIbX22wsl5nvjIj5KD+8twfeERFnZdVDtZXXMDO3i4h1gN9HxC3AgZn59SpOT8a8OPCJzPxGRLwDeCQznwQOiDIh2vnA5hHxvswc15NYTYypltD6BLBKZu5f3b4WpR/thyPiyJxDa4y6CsMfUA58bE6ptIWSuLy51aR1ZfOIuB94LDPfUcXcFdguIrahVPP+pD8nrSu1JNoBlKr3BSnvywvVNrlElrYrrVT9z0NJHtcnrN9CqSA+vC2jr1TJqoczc59q3LtQqjo3pRwQeaon68/MCRHxd0oS+UKqFi49G3URER+hnOFzD7AxZcK991MO6KxO6Tl+bTtiVfGi+gzOQzkDY6vMXD8iVs7Mh6qzGDYDPtvi+muf8RGUiWZPomxnGwOnUZKvv+npgZEq1owTmtYqhy8CLuhC0norygSzu0XEnZR+1kTEB4Abm9kndfI7p3pcp7bpruw7q6T1IOAxSpX7Gpn5r4jYANgD2Kad8SRJkiSpUxpJXH+u7vJwSgLhNspp5/3R2sCg6kf1DZn594g4ENi7U1XWM6qS5/+gnD69FW9MlHZmRHy/1VPSe9EKwMsRsW2WySS/UyXhPkGp7OqRLJOvrQJ8DPhvROyemef0cLXHA2Mi4l3A6cCvI2IMpX/pdRFxEqUatbeS1sMzc2J19XHg4SohMykz746IT1Nez69ExH1ZtTKZYR21VhWDI2IUpcI2KVXX11RJwHkzc5+ejDUzf1VVCf85IpaiHAA4KyKuBN4DrNRbLQg6qUr2DKO0VnkrZWK2Haq7P0NpUXB5k1WitST3v4DPVfuaS7L0UD8KuCPb07O9liAdAgwFFoiIJTLzTuDOiHg/sAWl+rXHqrNGvkobW7hUibbFKInq8ZTk5n+A06O0PtkJ2I42Jq7rHEiZvPTFalt/rEo2rwF8pwcJ0drjjqB83/wZICLuBdahPJ8bgR4nrjPzqqriej3emNB0JKUq+aqerr8FE4ELq33ZdZl5R/U+Hgts3eS6VqCD3znQmW26t/ed8cYZXQcBa1FaxoyhtH2DkrD+bfawJY0kSZIk9ZY5tgr5fw8oEz+dkpkf6cyQOici3k7pkXkbsAzwMPAsJSl1a2ae2GI1ZStjOZaSFDu1+nG7OQ+OMToAACAASURBVPDNbGLSt26IiA8BJ1NOP98Q+DPw5cxsS/JgJvGGUfp/tnwafZTJsb4AzE858HIe5aDNcpSE0WvA7sCmmflqjwfd2Jh+BtwFnErpcX4OZWLOWsuaMylV+btQkpuz7Ecab7RUuA+4mlJhOZnS4/jfmfloG8e9DvB7SqXop7JDEwr2pij9xUdn5mnV9S0oPYHvBL5MqfT9EbBhVZk5x31E3an6gyhnVCQlWbcRsG612NDMbEvlY128Myl9bAcDCwM/y8zv1i/TjnidEBHDMnNSlXTbjVL1ejolcfxCtcxKMzuI04OYQdkXXE5J7L/eB7pdBzKrbeAsYGfg6Mz8ZnX7MGCpdn4+q/V2ZZLemYxjMcrz3gh4T2beHBFnAfdm5teaWE+vfud0Sm/tOyNiIUp7oJ0pr9ufM/P0qCb/7URMSZIkSeqUVhLXAdydmWt2ZkidExHLUxKC81CqKqdRkpcPZdU6pIOxa4mlrSjJxW0pEzLeB7xCSWad2I0EQzMi4jTgD5l5ZUSsSqkk3Aw4IzNP6avJsSp5fQewBOVU6b9STtcfTempfUNm/qaXxjIfJYn5AUpF5LcpFblfBt5O6XX9AqXf6t+A92bmUzOso76lwq+BXSkTva1L2Z7GUCp7e9IiZFbjD0o1/C8pFYS98rp1SlUJOYTy2i+TmSdV7Qz2pewjpgPnZOb50eCEjHUV0EdREoijKAdM7qFMdDsfZXK6drSIqMVajLJ/27lKAL+bclBuBUrLnb/2NFanVJ+JgylVujtRDtoMorwHSwAXZ+ZPOjyGbYEzKK1U9s/Mf7ZpvctSPqsbUA6AvErpn93JyXN7bULTupi17XAQMDwzX4uI1SgHITYHJgATMvODTa63X37nzExv7DurfdeqlN7Wv6prUfIb4LjMfKjdMSVJkiSpUxqZnPH7vNH7dRAlMfZYtnkirk6LiKG1PpJRJie6MzOvqK7Pk5mT21VhN5PYtdN3N6ZUwr0/M1+O0lf1w5TJmsZm5i3tjt1OEfFeSuuYa4DTspogrWpDMF8b2nl0VETsTnmtP0NpRfDFzLy5i+M5mnIAYyhwE2VCwPFVwmcIJUEzKjOPmeFxgyg9XmstFR7JagLVupYKawCfzcz/dnD8Pa6G77aI+BwwMjOPi9L7/ARKYvFoysGteSmTpb7WxDprB6k2AH5KOUjxD2CnzLwpIhbPzGc68FyOpGwTp1BamoyvDtjsT2kXcXe7Y7ZTte1eRjmY89HMvKFq17EV8C3g4Ha1vKhLsq5M6f09HbgnM2+LiMMpvagPzMwzWlx/bZ+/G+XzuArlM/5rysR/J1BX3T03iYgvUNpgPERJ0D5LaeexAPB4NjfhbL/+zpmVdu876//vUh20OItyQPaAzLw+Ij4FfLBdZ3hIkiRJUm9pJHG9Z93VqZSk9Y0dHVWbRcRywJ6UpOX5wN6UibK+lpnn9GJ7kL8BX6H0IN6H0p/zq5l5aadjt0NEvJVy+vFqlFORbwLuyzd6Nff5dgQ1EXEw8GngAUpiaWIvbQO1hNkulOTzIcCbKBWJmwPXUQ5uTAQWqCVqZlhHr7dUmBtVBwAupVTBvoXSLuJ2Sr/jvavLJ2aZpK/h7boucb0/pfr9WUprgB0jYkVKv/VPZeYrbXoetXjbUCasGwVcQqm6f6Q/fB5rqgTbYEpV6lPAQZTt+9hOJN0i4lrgUUo18HTK99tJVbuFaT19j6p9/g6Uif42B4ZRWl6cC4zJXp4wsVOitNd5kbK9/57yfbs7JWF/M+Vzdm82OQnh3PSd00l1B0r2AKZQzipal7IfuIPy/4092nUWgSRJkiT1lkYS18MpPVOhtNSYOLvl+6Kq8nED3jhd+VlgP0rLjs9l5r97YQyLUBJkX6MkxaZSJuV7L/DJLJOQ9QsRsSklKQGlX+fZzVSk9hURMS+we6tVlT2M/RVK9eHPqsr7ZSktQyYB+8zq9ewLLRXmJhGxK29UqG9d2xdExJKUg0yvZubhTaxvRJY+2KMp1fSrUyp6t8/Mf0XEdyhtFA5q8/MYCayQZWLPXSgVry9QDpJd1t8Se9X+8iBgL2Ac8Pksk/K1Y921RP+OwJ6ZuV1ELEiZkPNo4NuZeXVPz8CJiC0pFfDfofQZXjci1qP0oN+zXdXj3VadHXIk5f8JSdmvfaW6b33KgaClgd1abY0zt3zndEJELJiZL0XEhyntpi6lTMK6MbAo5YDcdX39jC5JkiRJmplZJq6rH6Nfp1QFP07py7os8HPgmGYrp/qK2mn6EbEGJdm3NLBv9sKkfBFxCPBZ4FeU13YYcANlYrjJnY7fqohYmzK54ZOUxOq5wD8p1b7LZubXuzi8fikitgNOo7QLuLC67VfAbzPzstn1Uu7Nlgpzq+qgxSDK9nwhpef0ZZQq6zG1Fiu1FkONJDHrDipMpZzRcQglebQdcD1lssRtgXdm5stteA5DMnNqRHyWUtm6IfCvzNwrIpamVPTflpl/6mmsbqkqn1dpd9KtatWwHWXy0/3qzlT4POVMh2Nm9/gm4owA1qP0q9+e0ibkgP7WamtOqjMJ1gHeRXldvwWclZnjqvvXycw7m1if3zkNiIgVqNqoACsBZ2bmP6oDP+8B1gK+lJlTuzZISZIkSeqB2SWuv0s55fwzdT8+56f0/pyQmYf22ijbrP7U4oh4CHhfZj7QgTi1thCL8kbv4loibDglUXZOX+9zGhFXAD8DtqRU1T1DaUPwa+DZ6jl2pD/43GJmr0+USbT2AhYEbgV2yAYnPe3tlgpzm4g4FPgLZVteh5IUO5BSIX0Hparz5mYPKFUHFS4B5qf0lL05Ij5IqXxcDfhFZt7VxuexMKWqeltKZe/tWSasWyv7eE/rbqrekx2BhSgHZm8Dzqa8lidn5vltjDWY8t6sQvmMHpp9fBLeRs3YpiMilqFsi2+lTGx6dWZe1MJ6/c5pUJRJWPemJKpPz8wv1d13O2Ui0Ou7NT5JkiRJ6onZJa4fBFad8RTz6kf4fZm5Si+Mr2MiIoDFKdV2x3dg/bWk9XKUFiH/pVRiXkfp1TmOMnnece2O3U4RsSGlgnfPiPgHcAAlMbED8MPM/GlXB9hP1LUnOIYyGeOawG8oSdL3U9o6jMnMh2dXbT2T9XaspcLcLCLWyMx7I+J4YCzwx+pMjNGUNkITMvOwFtddO6iwC6V/8sGUpOUumfmZNo1/vaqycjSwBXAl8IPMfEd1/4WUyUftaVuZ4YDlApSzX4YBdwLbUM4ourld1dYzxF4YeDNlQsFr2r3+bqnrrXwsMG9mHl0dlF2Nsl1uCpySmX9rYp1+5zSpOoNgd0rLlnuAUyn7oM9n5lbdHJskSZIk9cTsEtcPZOaqzd6n/xUR5wN/AJai/LC8GZgMXJSZf+nm2BoRETtRfgiPpJzi/omIWAv4PCXpP9nJsWav7iDGO4ATgGMpkyl+nJI0G9bT3vGdaqkwN6p7PzYFNqNUdE6g9Ly/IjNfjYhlMvOJnlR1VgcVDqZMVDcRODzbMBFrVWG5K2WiujsobSjWBXbOzFsi4gDgQ5n5np7GmhtFxKcpVfFDgFMobbD2Ap4Hprgva0zd52gJSqX6Dpn5SETsSzkYdDewVGbe3OR6/c5pUfU98FngCEqV+ocz87bujkqSJEmSWjdoNvfdE2WG+v8REbsB93VuSHOPiFiNUrn5W+DDlMrYiymVaMt0cWgNiYhtgA9ReucuDIyOiIOB71Gq7idXyQsTCLNRl/jcnjL52yjgrsy8A1geOKqqUuxJjBdNWs9ZXbJtBeDrmXkicDjwMKUv9fERsW5mPgH/8941LTOfq86oWB/Yox1J68p9lN74bwU+CjxA2Zd/PSK+TKkYP6JNseYqVSuXQyh9zUcDv6Wc/XAqsKj7ssbVfTa2oRycfSkivko5QPsV4G0tJK39zumB6nvgi5TJqI8zaS1JkiSpv5tdxfXSlB+jEyj9P6H80B9BqeYb2ysj7MciYiSlt+1g4FuZ+ZHq9qsok+o9383xzU5VRfdrSluL54G/A8tRWh78MTNP7+Lw+o2ImC+riT+rntb7UBJlG2fmSxFxOjCx1bYUak2VDFukvlVPvDFh6w8z85FujW126iZk3I7Sk3tx4FpKK6KVKJ/TazPz4e6Nsm+LiPcBXwTup/RRfl/1t0lmvtbNsfUXEfFe4N7MfKw6CPQnSqui32fmdyNif2D1zDy8iXX6nSNJkiRJ+h+zTFy/vkA5LX2t6uo9mXlVx0fVj9VVdI6oblogM5+OiOsolbZPAvdn5me7N8o5i4gfUxITJ0fEBsDOlMrRiyjJiSdqSbSuDrQPi4i3UiqsT6NM9jcFOBlYDzgfeI7S5mHDzJzg6e+9IyKWpSTFXgL2zMxbuzykplW9fz+WmfdHxNbAJ4D5gL9k5ve7O7q+KSI2pxx8vZKyH94buD7LBJojM3N8VwfYT0TEPJT+7b+hfC9cTWm3snhm3hERy1Mq2j+eTUwQ6neOJEmSJGlGQ+a0QGZeTflhqjmoEo/Tq7YPv6ZU9K0XEftk5uZV70+APj25VEQMBV6lTFxGdbrxbRFxDrAlsARwtAmEOXoAeBw4DhgD/Bz4PiUZ80FKy4cDq6R1wxMyqmcy8z9Vcu0zwLkRcTalbci4Lg+tIRGxOPAUpd3Q/Zl5RUTcBZwLOBnjTFST8Y6iTJB7OqU6eDxwYETslJljujm+/qRq13E2sCBl8sUtKZMO/7laZCfKHA7NJK39zpEkSZIk/T9zrLhW4+qqrX9IqegbC+yTmZtFxJLAK/2lqi8iRlOqhS+iJF3vo1QNf4KSiD0sMx/r1vj6uvpEdEQsCHyZcubCOcClmflkN8c30NTej4jYEFiRMvHbVZRJE79L6dO7ZWb2i8RvROwNvJvSzukGYHXgq5n57q4OrJ+IiO2BVYGtge/0h4ly+4IZzwqpvte2oCSwp1MmN72olbNH/M6RJEmSJM3IxHWbVX2tz8jM3SLid8DFmXlWRHwSGJKZP+zyEBtSVShuDbwL2IRyKvj5wF+AczNznS4Or9+IiBOB8Zl5fES8CziUUvV5LuX0d6usO6zugNJCwCXAPZQ2LWsB52fmLyJi68y8oqsDbULVrmFPYA1KReozwMmZeVlXB9bPRMTQzJzS7XH0F3UHgNYHFgImZ+YNVZ/r91MOppzYSvsdv3MkSZIkSTMycd0GETEEGJWZL1bXvwB8FPhvZm5d3XYnpS3E37o30uZVifj5KBWqTwLnAT/KzEu6OrA+rL7aMCLeDBwA3JSZf6huOxyYv35iQHVedRBhcGZ+LiIWANYFDgKOyMx/V8v0qz7j1fMYRdmX/6fb49Hcq/bZqJLUf6N8F7yZMjHoNzPzvohYq5kWIbOI43eOJEmSJAkwcd0WEfEZSoXYKZRJ30YC3wIWAR6kVKYNy8y9uzbINoiIwcBWmXl5t8fSH0TEe4HLgF2Bj1AqrH9T3VdLAg3KzOndHOfcrH7SvYjYlTIR5mF19/8cuCUzf9StMUr9Qd2ZC/sB82bm96oDcztTKqXHAEe18ywSv3MkSZIkaWAzcd0GETEf8Flgc+Bm4FfAIGB5YFtK8vKGzHy1a4NUr4qIt1MmLPsJcCmlrcNWwBnACZR+5374Oigi5gcOBP5KmbRwFKXH+MPAz4DbgFuBnTLz7v5WbS31toh4G/AjSjus06v2HsMpZy+slZk/6eoAJUmSJElzFRPXPVRfMRsRqwCfpkz6dTZwUWY+183xqffM0CJkOPAlSt/XEymnvJ8IvJiZ7+veKAeOiFiZMtnbJOAflF65gygHkz5LqRC9u+o/buW7NBMRsTCwcmbeUl3+HuUMo+/UzlSoEth44EeSJEmS1E4mrnug7tTpEcCbgEWBOymV14cAIygTpvWbSd/UcxGxLTBfZp4fEZ8ANgOOzMxnI2LFzHy0NslZl4c6IETE8cDSlF68twJ/z8z/RMSwzJxULWPiWpqJiDgQuBEYDwyvzk7YhNIOC0oC+4KuDVCSJEmSNNcycd0DdX2KT6JMJrU88I/M/EJ1/6eAf2XmDd0cp3pPRMwD7A28E1iN0pLiXcBkYA+T1b2jdmAgIrai9OD9F7AyMC/wOHA7cE1mvtbFYUp9WkRsAawO/JiSqF4cuAK4IDNfjoj9gb2ATa22liRJkiS1m4nrHoqI9Sl9i7eg9DQ+KjOvjIgNM/PWrg5OXRERC2XmixGxJvAeYDngk8DbM/OO7o5u7ld3JsSSlH7zwynJ6jurRVYHxmbml7o1Rqk/iIjrKfuueYCk9LLeEngJuDwzL7E3vCRJkiSpU0xc91BEvBdYAngK2Dszd46IBYFzgd0z85muDlAdV1fduxlwEDAM+A9wRWZeVPV/nSczJ5nk6T0R8WPg3sw8OSJGAzsB7wDOAq6rWh4MycypXR2o1AdFxDDgeMrBt6Uyc9Hq9qWBXSmfpYsz88zujVKSJEmSNDczcd2CuorOjwHbA8sCbwXelZm3R8TJwIjMPLCrA1Wvioi7gS8CrwGjKAmfn2XmTV0d2AAUEUMprQ2ezcxv1N1+DqX3/D2ZeVS3xif1FxHxAOWshWuAb2fmXdXtGwNPZea/uzk+SZIkSdLca1C3B9Af1U3ithnwHWBP4GLg6xHxS+DtgEmxASQiVgUeysw/ZublwOWU9hR7RcSwqupavSQzp1Aqq0dHxF4RsXZEDAFWAb4ErB4RK3RxiFKfVn1eAA4D1gbGAr+LiBMjYt7M/LtJa0mSJElSJ5m4blFEfAA4ENg2Mx+h/Lg/EfgjsENmvtzN8anXPQssEhHfj4ilMnMccCmwbGZOsj1IV9xG6T+/GvAD4Crg58AEYMXMfKx7Q5P6ptpBtsycGhHLAs9l5ivVpMO7Am8Gro+I4d0cpyRJkiRp7merkBZFxDzAXsDhwD+B4zLznq4OSl0VESsCe1OqesdX/56ZmWfV+mB3dYADVESMBOYDRgJPAucBP8rMS7o6MKkPqmuFdTSl3dE0SquQL2fmldUyq2Tmg90cpyRJkiRp7mfiuociYiHgEOAjwK3AJ01Qzv3qJmR8M/BJShXv7ykJnmHARsDNmXljF4epGUTEYGCrqp2LpDq1yWMjYgTwJ+DTmXlfRHyS8j33b2D/zHyiqwOVJEmSJA0IJq7bJCLWoiTEvtftsaj3RMRfgDuAFYHpwNXAZZn5eN0yYasQSX1dXeJ6L2AL4Hjg0aoCewHg28AfMvOyLg5TkiRJkjRAmLiWmlSX3Hk7cHhm7ljdvjWwB7Ao8MXMHNPNcUpSo+r2a0sAPwXmB64EfgM8kZmTujpASZIkSdKAM6TbA5D6kxmqpzcENoqI/YCfZeYVwBURsQdwd9cGKUlNqtuvbQPcCQwF3gmsAVwUEddm5thujU+SJEmSNPBYcS01oa4q8XPAwsDTwFrAC8DFmfnXGZft0lAlqWF1kzKuB2wMLAIksASlbcg3MvO3XRyiJEmSJGmAMXEtNSgi3pSZz1eXPwQsB5wObEJJ9KxOmZDxjO6NUpJ6LiLeCewELAXcDpycmRO6OypJkiRJ0kAyqNsDkPqDiAjg/Ij4RUQsBtwIbFT1ff0b8HfgQeAvdctLUr8QER+NiAerg3Jk5vXAscAo4FqT1pIkSZKk3maPa6kBVXuQ/YBDgCsoE5YtGxEnA2sC44AXMvPR2vJdG6wkNe8PwAjgsxGxGyVpvTgwNDNv7OrIJEmSJEkDkq1CpCZFxGbAYcB2wOXATpk5se7+QZk5vVvjk6RWRcTCwKcoB+luA07NzMu6OypJkiRJ0kBk4lpqQUTMA7yfksCeCOwLPJmZ07o6MElqg4gYBiyYmc90eyySJEmSpIHJxLXUA1V14j7AKZk5pdvjkSRJkiRJkuYGJq6lNrFFiCRJkiRJktQeJq4lSZIkSZIkSX3KoG4PQJIkSZIkSZKkeiauJUmSJEmSJEl9iolrSZIkSZIkSVKfYuJakiRJqhMRr3ZgnStExMdncd+giDg1Iu6KiH9FxK0RsWK7xyBJkiT1J0O6PQBJkiRpAFgB+Djw25nctzOwFPDWzJweEcsA43txbJIkSVKfY8W1JEmSNBMRsUVEXBsR50fEfRFxVkREdd9jEfGtqkL6lohYubr9FxGxY906atXbJwKbRcQdEfGZGUItCTyVmdMBMvOJzHyxevw2EXFTRNweEedFxHzV7dtWY7q9qta+uLr9uIg4oi7+XRGxQnV5t2qsd0TEGRExuDbGiDghIu6MiJsjYvHq9sUj4o/V7XdGxNtntx5JkiSpnUxcS5IkSbO2HnAYsCawErBp3X0vZ+ZbgB8Ap8xhPUcBN2Tmupn53RnuOxfYrkoEfyci1gOIiEWALwJbZeb6wBjg8IgYDvwY2A7YAFhiTk8iItagVHZvmpnrAtOAXau7RwI3Z+Y6wPXAftXtpwLXVbevD9w9h/VIkiRJbWOrEEmSJGnWbsnMJwAi4g5Ky4+/Vvf9ru7fGZPRDcvMJyJiNeDd1d9VEbETMIKSML+xKvSeB7gJWB14NDMfrMb1G2D/OYTZkpLkvrVa1wjg2eq+ycDF1eXbgK2ry+8G9qjGOA14OSJ2n816JEmSpLYxcS1JkiTN2qS6y9P43/8/50wuT6U6qzEiBlGSzXOUmZOAS4FLI+IZ4IPAX4ArMnOX+mUjYt3ZrOr1+JXhtYcBv8zMo2fymCmZWRv/jM9xRrNbjyRJktQ2tgqRJEmSWrNz3b83VZcfo1QkA2wPDK0ujwNGzWwlEbF+RCxVXR4EvBV4HLgZ2LSuf/bIiFgVuA9YISLeXK2iPrH9GKWtBxGxPrBidftVwI4RsVh138IRsfwcnt9VwIHV8oMjYoEW1yNJkiQ1zcS1JEmS1JqFIuKfwKFAbcLFHwObR8SdwCbA+Or2fwLTqkkOZ5yccTHgooi4q1puKvCDzPwvsBfwuyrOTcDqmTmR0hrkzxFxO//bquP3wMIRcTdwMPAAQGbeQ+mX/ZdqXVdQJoWcnUOBd0XEvygtRNZscT2SJElS0+KNswIlSZIkNSIiHgNGZ+ZzfWAsWwBHZOYHuj0WSZIkqV2suJYkSZIkSZIk9SlWXEuSJEmSJEmS+hQrriVJkiRJkiRJfYqJa0mSJEmSJElSn2LiWpIkSZIkSZLUp5i4liRJkiRJkiT1KSauJUmSJEmSJEl9iolrSZIkSZIkSVKfYuJakiRJkiRJktSnmLiWJEmSJEmSJPUpJq4lSZIkSZIkSX2KiWtJkiRJkiRJUp9i4lqSJEmSJEmS1KeYuJYkSZIkSZIk9SkmriVJkiRJkiRJfYqJa0mSJEmSJElSn2LiWpIkSZIkSZLUp5i4liRJkiRJkiT1KSauJUmSJEmSJEl9iolrSZIkSZIkSVKfYuJakiRJkiRJktSnmLiWJEmSJEmSJPUpJq4lSZIkSfo/duxYAAAAAGCQv/U0dhRGAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAGg2q4wAAIABJREFUYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAMCKuAYAAAAAYEVcAwAAAACwIq4BAAAAAFgR1wAAAAAArIhrAAAAAABWxDUAAAAAACviGgAAAACAFXENAAAAAEDt3Xu0bVddH/DvjwRMigLFqEMgg8irEQhcIURawQaFEHyUIFCMEYhl0EZKH7TQxo6CyHCMQumQIUJFMRCDUFLAFEQhoAJChMAN5AVpeBlEoIUbkUdMELi//rHWSU7O3efs89j7nHV7P58xzrh7r73WXHOuveZe+37PPHNNiuAaAAAAAIBJEVwDAAAAADApgmsAAAAAACZFcA0AAAAAwKQIrgEAAAAAmBTBNQAAAAAAkyK4BgAAAABgUgTXAAAAAABMiuAaAAAAAIBJEVwDAAAAADApgmsAAAAAACZFcA0AAAAAwKQIrgEAAAAAmBTBNQAAAAAAkyK4BgAAAABgUgTXAAAAAABMiuAaAAAAAIBJEVwDAAAAADApgmsAAAAAACZFcA0AAAAAwKQIrgEAAAAAmBTBNQAAAAAAkyK4BgAAAABgUgTXAAAAAABMiuAaAAAAAIBJEVwDAAAAADApgmsAAAAAACZFcA0AAAAAwKQIrgEAAAAAmBTBNQAAAAAAkyK4BgAAAABgUgTXAAAAAABMiuAaAAAAAIBJEVwDAAAAADApgmsAAAAAACZFcA0AAAAAwKQIrgEAAAAAmBTBNQAAAAAAkyK4BgAAAABgUgTXAAAAAABMiuAaAAAAAIBJEVwDAAAAADApgmsAAAAAACZFcA0AAAAAwKQIrgEAAAAAmBTBNQAAAAAAkyK4BgAAAABgUgTXAAAAAABMytF7XQHYLac9+vS+/sCBm5/3HtXjkP0uoCKHFrH41vUuHLBN7WIJ9Zhf5DZ22hs+3TVz97snx3MbO56x+l4c0+2do73xy8uqx5yNFnHeL6UtOz81tlzIdt7X7bR9q/1xO5/t2+nzW23LIj6rt3X8tngh2t4+trHNAjboHfamXan3jB0t5DNgGXWfU+he1fuQMpbwZWEx3zM38T7vsK6LKXMTn5KLuKZMoF6b2usyvqzP/SzfnQ+fufuZaH/czJeHrZ9eS/j2tYz/hG2rnot4n7dTxg6/CS3rP7Hz/k9x45cu7u7Tt14wHF4E1xwxrj9wIJdcuv/m52uvpTO/EM25Js0tYxNfTuaVsZlr/lbL2Mz1ei/KmN3WnZcx733Z8vu6iDIWcG4so4xZb8EUyljG+7qdMg5uIrSZ19aD2+jzCyljzbKDc/vW/DLnl7G2DocWcnDOOvPKWLv9zDK2+L5uZ7+HHN9ZZc5Z55D3dc57NrvMjfcx8308ZL8b12ve+Te7zI3rtXb9zWxz6PHZeP3ZZa59fX695m5zyD438x7MOWfntX075/CcfWymHvPKmPUZudO2bqeMzZyzO23rrDIW0da5n2eHvL6MMhdfxnbOt82VscO+NKPT707b5nwf2VRbl1+mei22zM1ts/b5wY1f38w6h7y+jDKXUMba15MZH8570LZZ9dqFMm66/OXHHboR/P/HVCEAAAAAAEyK4BoAAAAAgEkRXAMAAAAAMCmCawAAAAAAJkVwDQAAAADApAiuAQAAAACYFME1AAAAAACTIrgGAAAAAGBSBNcAAAAAAEyK4BoAAAAAgEkRXAMAAAAAMCmCawAAAAAAJkVwDQAAAADApAiuAQAAAACYFME1AAAAAACTIrgGAAAAAGBSBNcAAAAAAEyK4BoAAAAAgEkRXAMAAAAAMCmCawAAAAAAJkVwDQAAAADApAiuAQAAAACYFME1AAAAAACTIrgGAAAAAGBSBNcAAAAAAEyK4BoAAAAAgEkRXAMAAAAAMCmCawAAAAAAJkVwDQAAAADApAiuAQAAAACYFME1AAAAAACTIrgGAAAAAGBSBNcAAAAAAEyK4BoAAAAAgEkRXAMAAAAAMCmCawAAAAAAJkVwDQAAAADApAiuAQAAAACYFME1AAAAAACTIrgGAAAAAGBSBNcAAAAAAEyK4BoAAAAAgEkRXAMAAAAAMCmCawAAAAAAJkVwDQAAAADApAiuAQAAAACYFME1AAAAAACTIrgGAAAAAGBSBNcAAAAAAEyK4BoAAAAAgEkRXAMAAAAAMCmCawAAAAAAJkVwDQAAAADApFR373UdYFdU1duTHLfX9TgCHJfkwF5XAiZAX4CBvgADfQEG+gIMdtIXDnT36YusDEyR4BpYqKra390n73U9YK/pCzDQF2CgL8BAX4CBvgDzmSoEAAAAAIBJEVwDAAAAADApgmtg0X57rysAE6EvwEBfgIG+AAN9AQb6AsxhjmsAAAAAACbFiGsAAAAAACZFcA1sqKpOr6prq+qTVXXujNe/o6ouHF+/tKpOGJd/d1W9q6q+XlUvW7PNg6vqqnGbl1ZV7U5rYPuW1BfePZZ5+fjzvbvTGti+HfSFR1XVZePn/2VV9WOrtnFd4LCzpL7gusBhZwd94ZRV5/oVVfW4zZYJU7SkvnDdeL24vKr2715rYBoE18C6quqoJC9P8pgk901yZlXdd81qT0vy5e6+V5KXJHnRuPymJM9N8uwZRf9mkqcnuff4c/riaw+Ls8S+kCRndfe+8eeLi689LM4O+8KBJD/d3ScleWqS16zaxnWBw8oS+0LiusBhZId94eokJ3f3vgyf+79VVUdvskyYlGX0hVXbPWK8Jpy81EbABAmugY2ckuST3f3p7v67JK9P8tg16zw2ye+Oj9+Y5Merqrr7hu5+X4bQ7mZV9f1J7tDdH+hhkv0Lkpyx1FbAzi28L8Bhaid94SPd/flx+UeTHDuOPHJd4HC08L6wK7WGxdtJX/jb7v7WuPyYJCs34NpMmTA1y+gLcMQTXAMbuWuSz656/lfjspnrjBfbryT57jll/tWcMmFqltEXVrx6/NO/55oegcPAovrC45N8uLu/EdcFDk/L6AsrXBc4nOyoL1TVD1fVR5NcleSc8fXNlAlTs4y+kAwh9jvGqaX++RLrD5MkuAaAvXPW+KfiDx9/nrzH9YGlq6r7ZfjT2H+x13WBvbROX3Bd4IjS3Zd29/2SPCTJL1XVMXtdJ9gLG/SFh3X3gzJMQfIvq+pH96ySsAcE18BGPpfk+FXP7zYum7nOOA/XHZNcP6fMu80pE6ZmGX0h3f258d+vJXldhj8xhCnbUV+oqrsluSjJU7r7U6vWd13gcLOMvuC6wOFoId+RuvuaJF9Pcv9NlglTs4y+sPq68MUM1w3XBY4ogmtgIx9Kcu+q+oGqul2Sn03yljXrvCXDjYWS5AlJ/nSco3Sm7v5Ckq9W1UPHP399SpI3L77qsFAL7wvjzYeOGx/fNslPZbgxC0zZtvtCVd0pyR8mObe7L1lZ2XWBw9TC+4LrAoepnfSFH1i5AV1V3T3JiUmu22SZMDUL7wtVdfuq+q5x+e2TnBbXBY4wR89fBThSdfe3quqZSS5OclSSV3X3R6vqBUn2d/dbkpyX5DVV9ckkf53hAp0kqarrktwhye2q6owkp3X3x5I8I8n5SY5N8rbxByZrGX0hyWeSXDyGE0cl+eMkr9zFZsGW7bAvPDPJvZI8r6qeNy47bRxB5LrAYWUZfSHJDXFd4DCzw77wsCTnVtU3kxxM8ozuPpAks8rc1YbBFi2jL1TVPZJcNN7u4Ogkr+vut+9uy2Bv1QaDwQAAAAAAYNeZKgQAAAAAgEkRXAMAAAAAMCmCawAAAAAAJkVwDQCwi6rqjKrqqjpx1bITqmrDu8RvZp1Fqqqzq+plCyqrqupPq+oO4/NvV9XlVXV1Vb2hqv7eFsv7+hbXP7+qnjBj+clV9dLx8c3trapzquopq5bfZSv726qqOrWq/tEOy/hP29jmiVV1TVW9a83yE6rq51Y939G5MB7/U6vq3VV1wja2P3E8Xz5SVQ+uqmdsty5b2Ofzx3afX1WnjsteX1X3Xva+AQAYCK4BAHbXmUneN/57pPiJJFd091fH5zd2977uvn+Sv0tyzuqVx6B76d9Tu3t/d//rGctf0d0XjE/PTrLU4DrJqUl2FFwn2XJwneRpSZ7e3Y9Ys/yEJD936Op75owkb+zuH0pyfZKlB9fr+M0k/2GP9g0AcMQRXAMA7JKq+s4kD8sQGP7sOuucXVVvHkenfqKqfnnVy0dV1Sur6qNV9Y6qOnbc5ulV9aGquqKq3rR2BHNV3aaqrquqO61a9omq+r6q+umqunQczfrHVfV9M+p0qxHLq0c8V9Vzxn1fWVW/sk7Tz0ry5nVee2+Se42jfK+tqguSXJ3k+Ko6s6quGkdmv2hNnV4yHoc/qarv2cRxeGRV7a+qj1fVT43rn1pVb53R3udX1bPHNp+c5LXjiN+frKr/tWq9R1XVRTO2//HxeF5VVa+qqu8Yl19XVceNj09eNQL5nCTPGvfx8PF4v2JGfW818rmq3jq24YVJjh23f+2M+hxyHKvqeRnOxfOq6sVrNnlhkoeP5T1rXHaXqnr7eN7811Vln1ZV76+qD9cwev471+4/yVcy/ILir5N8u6qOGtt49VivZ41l7auqD4zn0kVV9fer6ieS/Nskv1jDyPAXJrnnWLcXj+1/z9hnPl1VL6yqs6rqg2PZ9xzLnnmeV9Wvj8ciVfXoqvqzGn5p8vUkN66qezKcq4+sqqNntBEAgAUTXAMA7J7HJnl7d388yfVV9eB11jslyeOTPCDJE6vq5HH5vZO8vLvvl+RvxnWS5Pe7+yHd/cAk12QIxm/W3QczBMePS5Kq+uEkn+nu/5th9PdDx9Gsr88WRpRW1WljnU5Jsi/Jg6vqR2es+iNJLpux/dFJHpPkqlXt++9j+76Z5EVJfmws+yFVdca43u2T7B/Xe0+SlXB/o+NwwljPn0zyiqo6Zl77uvuNSfYnOau79yX5oyQnrgTlSX4hyavWtOmYJOcneVJ3n5Tk6CS/uME+rkvyiiQvGUehv3er9e3uc3PLKPaz1tTnLplxHLv7Bava9pw1RZ6b5L1jeS8Zl+1L8qQkJyV5UlUdP4bw/znJI7v7QWN5/25G/f5Nd/95d/9Md392LOuu3X3/8Ri9elz1giT/sbsfkOGc+OXu/qNVx+dMJHXFAAAFS0lEQVQRY90+NdZtpd4PzBD+/2CSJye5T3efkuR3kvyrcZ31zvNfGtvziCQvTfIL3X2wu/9bd1+4UvexHQeTfHLcHwAASya4BgDYPWdmCM0y/rvedCHv7O7ru/vGJL+fYWRskvxFd18+Pr4sQ7iZJPevqvdW1VUZRjffb0aZF2YIHpNhtPeF4+O7Jbl43PY562y7ntPGn48k+XCSEzOEz2vdubu/tur5sVV1eYag8y+TnDcu/0x3f2B8/JAk7+7uL3X3t5K8NslKKH5wVf1/L7ccn42Ow/8cA8lPJPn0WNct6e5O8pokP1/D6PV/mORta1b7Bxnep4+Pz393Vb23Ysf1HW10HLfiT7r7K919U5KPJbl7kocmuW+SS8b386nj8nk+neQeVfUbVXV6kq9W1R2T3Km73zOus5Xj9qHu/kJ3fyPJp5K8Y1x+VW7pIzPP8+7+2yRPT/LOJC/r7k/N2dcXs/ypYwAAyDACBACAJauqO2cY9XpSVXWSo5J0Va0d7Zokvc7zb6xa9u0kx46Pz09yRndfUVVnZ5gzea33Z5iS43syzBn8q+Py30jya939lhpuQvf8Gdt+K+OAh3EahdutNCvJf+nu35qxza22r6rbjCNWk3F08OoVqipJbphTznpWjs/5Wf84rHdMt+rVSf4gyU1J3jCGwZt183FMMm/E96z6rt5+M2Us0tpz7+gM7/87u3tL87V395er6oFJHp1hpPQ/TfKsjbfadN0Ornp+MLf8f2ej8/ykDHNnbyaQPibDFCIAACyZEdcAALvjCUle09137+4Tuvv4JH+R5OEz1n1UVd25hjmsz0hyyZyyvyvJF6rqthlGGh9iHC18UZJfS3JNd18/vnTHJJ8bHz91nfKvS7Iyrck/SXLb8fHFSf7ZyrzGVXXXqvreGdtfm+Qec9qw1geT/OOqOq6qjsowOn1lNO5tMhzPZLiJ4PvGxxsdhyfWMNf3Pce6XLvJenxtLDdJ0t2fT/L5DFNkvHrG+tcmOaGq7jU+f/Kqel+XW47j41dtc6t9bFDf65LsG5cfn2EqkRXfHNu91kbHcT2z6jPLB5L8yEpbq+r2VXWfeRuNU4zcprvflOE4Pqi7v5Lky1W10h9WH7ft1G2tmed5Vd09yb9P8kNJHjNOo7OR+2SYgx0AgCUTXAMA7I4zMwTHq70ps6cL+eD42pVJ3tTd++eU/dwkl2YIuP/3ButdmOTnc8s0G8kw8vQNVXVZkgPrbPfKDOHnFRmmx7ghSbr7HUlel+T94xQMb8zsUPEPM3sU+Lq6+wsZ5jN+V5IrklzW3Ss3eLwhySlVdXWGUewvGJdvdBz+MsNxfVuSc8YpLzbj/AxzTF8+/iIhGabb+Gx3XzOj3jdlmPv6DeMxOZhhjuYk+ZUkv15V+zOMWl7xB0keN+5jJbidVd9LMvyy42MZ5mP+8KoyfjvJlWtvzjjnOK7nygw3Ubxi1c0ZD9HdX0pydpL/UVVXZhjVv5kpTe6a5N3j9CK/l2Ge6WQIlF88lrUvt7yvq/d5fYapSa6uQ28quZHnZ815XsMw//OSPHv8hcTTkvzOevOJjzd0vLG7/88W9gsAwDbVMPgGAIApGKe4OLm7n7nXdVmUqvr+JBd096P2ui6LUFUvS/KR7j5v7srbK//8JG8dbw7JRIwh/leX9b4DAHBrRlwDALBU46jfV1bVHfa6Ljs1jth9QIaRwhxZ/ibDTSMBANgFRlwDAAAAADApRlwDAAAAADApgmsAAAAAACZFcA0AAAAAwKQIrgEAAAAAmBTBNQAAAAAAkyK4BgAAAABgUv4fysKFo6+qlsQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1800x1836 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "attention_map = plot_attention_map(model, X_test_indices[44].reshape(1,54), X_test[44], n_s = 128, num = 8, Tx = Tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"../models/model.json\", \"w\") as json_file:\n",
    "    print('works')\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"../models/model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coreml_model = coremltools.converters.keras.convert(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coreml_model.save('../models/CommentNetV2.mlmodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coreml_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sisira Dabare"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
